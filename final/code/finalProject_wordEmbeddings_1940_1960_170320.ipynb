{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "########## COUNTING WORDS AND PHRASES ##########\n",
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All these packages need to be installed from pip\n",
    "import gensim#For word2vec, etc\n",
    "import requests #For downloading our datasets\n",
    "import lucem_illud_2020\n",
    "\n",
    "import numpy as np #For arrays\n",
    "import pandas as pd #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "import sklearn.metrics.pairwise #For cosine similarity\n",
    "import sklearn.manifold #For T-SNE\n",
    "import sklearn.decomposition #For PCA\n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning\n",
    "%matplotlib inline\n",
    "\n",
    "import os #For looking through files\n",
    "import os.path #For managing file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in overall dataframe\n",
    "movie_df = pd.read_csv(\"/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/movie_df.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-a247250f1706>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read in dataframes separated by year\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmovie_df_1940_2020\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/movie_df_1940_2020.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmovie_df_1940_1960\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/movie_df_1940_1960.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmovie_df_1960_1980\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/movie_df_1960_1980.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmovie_df_1980_2000\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/movie_df_1980_2000.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2060\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m     \"\"\"\n\u001b[1;32m    680\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# read in dataframes separated by year\n",
    "movie_df_1940_2020 = pd.read_csv(\"/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/movie_df_1940_2020.csv\") \n",
    "movie_df_1940_1960 = pd.read_csv(\"/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/movie_df_1940_1960.csv\")\n",
    "movie_df_1960_1980 = pd.read_csv(\"/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/movie_df_1960_1980.csv\")\n",
    "movie_df_1980_2000 = pd.read_csv(\"/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/movie_df_1980_2000.csv\")\n",
    "movie_df_2000_2020 = pd.read_csv(\"/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/movie_df_2000_2020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in genre dataframe\n",
    "movieGenre = pd.read_csv(\"/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/movie_df_genre.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check dataframe\n",
    "movie_df_1940_2020.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "########## WORD2VEC ##########\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df_1940_2020['tokenized_sents'] = movie_df_1940_2020['Text'].apply(lambda x: [lucem_illud_2020.word_tokenize(s) for s in lucem_illud_2020.sent_tokenize(x)])\n",
    "movie_df_1940_2020['normalized_sents'] = movie_df_1940_2020['tokenized_sents'].apply(lambda x: [lucem_illud_2020.normalizeTokens(s) for s in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load our data our data we give all the sentences to the trainer:\n",
    "movieW2V_1940_2020 = gensim.models.word2vec.Word2Vec(movie_df_1940_2020['normalized_sents'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieW2V_1940_2020.wv.index2entity[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df_1940_1960['tokenized_sents'] = movie_df_1940_1960['Text'].apply(lambda x: [lucem_illud_2020.word_tokenize(s) for s in lucem_illud_2020.sent_tokenize(x)])\n",
    "movie_df_1940_1960['normalized_sents'] = movie_df_1940_1960['tokenized_sents'].apply(lambda x: [lucem_illud_2020.normalizeTokens(s) for s in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load our data our data we give all the sentences to the trainer:\n",
    "movieW2V_1940_1960 = gensim.models.word2vec.Word2Vec(movie_df_1940_1960['normalized_sents'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieW2V_1940_1960.wv.index2entity[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(movieW2V_1940_1960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can look at a few things that come from the word vectors. \n",
    "# The first is to find similar vectors (cosine similarity):\n",
    "movieW2V_1940_1960.most_similar('right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find which word least matches the others within a word set (cosine similarity):\n",
    "# the fact that 'thank' matches least goes with themes of non-collaborative\n",
    "movieW2V_1940_1960.doesnt_match(['good', 'love', 'want', 'thank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the vectors for later use:\n",
    "movieW2V_1940_1960.save(\"movieWORD2Vec_1940_1960\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use dimension reduction to visulize the vectors. \n",
    "# start by selecting a subset we want to plot. \n",
    "# look at the top words from the set:\n",
    "numWords_1940_1960 = 10\n",
    "targetWords_1940_1960 = movieW2V_1940_1960.wv.index2word[:numWords_1940_1960]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract their vectors and create our own smaller matrix that preserved the distances from the original:\n",
    "wordsSubMatrix_1940_1960 = []\n",
    "for word in targetWords_1940_1960:\n",
    "    wordsSubMatrix_1940_1960.append(movieW2V_1940_1960[word])\n",
    "wordsSubMatrix_1940_1960 = np.array(wordsSubMatrix_1940_1960)\n",
    "wordsSubMatrix_1940_1960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use PCA to reduce the dimesions (e.g., to 50), and T-SNE to project them down to the two we will visualize\n",
    "pcaWords_1940_1960 = sklearn.decomposition.PCA(n_components = 10).fit(wordsSubMatrix_1940_1960)\n",
    "reducedPCA_data_1940_1960 = pcaWords_1940_1960.transform(wordsSubMatrix_1940_1960)\n",
    "#T-SNE is theoretically better, but you should experiment\n",
    "tsneWords_1940_1960 = sklearn.manifold.TSNE(n_components = 2).fit_transform(reducedPCA_data_1940_1960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "plt.scatter(tsneWords_1940_1960[:, 0], tsneWords_1940_1960[:, 1], alpha = 0)#Making the points invisible \n",
    "for i, word in enumerate(targetWords_1940_1960):\n",
    "    ax.annotate(word, (tsneWords_1940_1960[:, 0][i],tsneWords_1940_1960[:, 1][i]), size =  20 * (numWords_1940_1960 - i) / numWords_1940_1960)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "########## DOC2VEC ##########\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will load these as documents into Word2Vec, but first we need to normalize and pick some tags.\n",
    "keywords = ['good', 'bad', 'kill', 'save', 'stop', 'fight','beat','man','woman','miss','mr','mother','dad','good','right','wrong']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taggedDocs_1940_1960 = []\n",
    "for index, row in movie_df_1940_1960.iterrows():\n",
    "    #Just doing a simple keyword assignment\n",
    "    docKeywords_1940_1960 = [s for s in keywords if s in row['normalized_words']]\n",
    "    docKeywords_1940_1960.append(row['Year'])\n",
    "    docKeywords_1940_1960.append(row['Movie Name']) #This lets us extract individual documnets since doi's are unique\n",
    "    taggedDocs_1940_1960.append(gensim.models.doc2vec.LabeledSentence(words = row['normalized_words'], tags = docKeywords_1940_1960))\n",
    "movie_df_1940_1960['TaggedAbstracts'] = taggedDocs_1940_1960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can train a Doc2Vec model:\n",
    "movieD2V_1940_1960 = gensim.models.doc2vec.Doc2Vec(movie_df_1940_1960['TaggedAbstracts'], size = 50) #Limiting to 50 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the doc2vec model, the documents have vectors just as the words do, \n",
    "# so that we can compare documents with each other and also with words \n",
    "# First, we will calculate the distance between a word and documents in the dataset:\n",
    "movieD2V_1940_1960.docvecs.most_similar([ movieD2V_1940_1960['kill'] ], topn=5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "########## PROJECTION ##########\n",
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#words to create dimensions\n",
    "tnytTargetWords = ['good','bad','fight', 'kill', 'save', 'stop']\n",
    "#words we will be mapping\n",
    "tnytTargetWords += [\"man\",\"woman\",\"old\",\"money\",\"right\", \"know\",\"come\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsSubMatrix_1940_1960 = []\n",
    "for word in tnytTargetWords:\n",
    "    wordsSubMatrix_1940_1960.append(movieW2V_1940_1960[word])\n",
    "wordsSubMatrix_1940_1960 = np.array(wordsSubMatrix_1940_1960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaWordsMovie_1940_1960 = sklearn.decomposition.PCA(n_components = 10).fit(wordsSubMatrix_1940_1960)\n",
    "reducedPCA_dataMovie_1940_1960 = pcaWords_1940_1960.transform(wordsSubMatrix_1940_1960)\n",
    "#T-SNE is theoretically better, but you should experiment\n",
    "tsneWordsMovie_1940_1960 = sklearn.manifold.TSNE(n_components = 2).fit_transform(reducedPCA_dataMovie_1940_1960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see that good and bad are strongly represented, but that fight and kill are \n",
    "## also highly represented -- showing a strong theme of violence, with a nod to 'stop' as well\n",
    "fig = plt.figure(figsize = (10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_frame_on(False)\n",
    "plt.scatter(tsneWordsMovie_1940_1960[:, 0], tsneWordsMovie_1940_1960[:, 1], alpha = 0) #Making the points invisible\n",
    "for i, word in enumerate(tnytTargetWords):\n",
    "    ax.annotate(word, (tsneWordsMovie_1940_1960[:, 0][i],tsneWordsMovie_1940_1960[:, 1][i]), size =  50 * (len(tnytTargetWords) - i) / len(tnytTargetWords))\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some convenient functions for getting dimensions.\n",
    "def normalize(vector):\n",
    "    normalized_vector = vector / np.linalg.norm(vector)\n",
    "    return normalized_vector\n",
    "\n",
    "def dimension(model, positives, negatives):\n",
    "    diff = sum([normalize(model[x]) for x in positives]) - sum([normalize(model[y]) for y in negatives])\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculate 2 dimensions: violence and nonviolence\n",
    "violence_1940_1960 = dimension(movieW2V_1940_1960, ['bad'], ['fight','kill'])\n",
    "nonviolence_1940_1960 = dimension(movieW2V_1940_1960, ['good'], ['save','stop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words around gender and incentives\n",
    "gender = [\"man\",\"woman\"]\n",
    "incentive = [\"money\",\"know\",\"right\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to project words in a word list to each of the 2 dimensions\n",
    "def makeDF(model, word_list):\n",
    "    n = []\n",
    "    v = []\n",
    "    for word in word_list:\n",
    "        n.append(sklearn.metrics.pairwise.cosine_similarity(movieW2V_1940_1960[word].reshape(1,-1), violence_1940_1960.reshape(1,-1))[0][0])\n",
    "        v.append(sklearn.metrics.pairwise.cosine_similarity(movieW2V_1940_1960[word].reshape(1,-1), nonviolence_1940_1960.reshape(1,-1))[0][0])\n",
    "    df = pd.DataFrame({'nonviolence': n, 'violence': v}, index = word_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the projections.\n",
    "genderdf_1940_1960 = makeDF(movieW2V_1940_1960, gender) \n",
    "incentivedf_1940_1960 = makeDF(movieW2V_1940_1960, incentive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some useful functions for plotting.\n",
    "def Coloring(Series):\n",
    "    x = Series.values\n",
    "    y = x-x.min()\n",
    "    z = y/y.max()\n",
    "    c = list(plt.cm.rainbow(z))\n",
    "    return c\n",
    "\n",
    "def PlotDimension(ax,df, dim):\n",
    "    ax.set_frame_on(False)\n",
    "    ax.set_title(dim, fontsize = 20)\n",
    "    colors = Coloring(df[dim])\n",
    "    for i, word in enumerate(df.index):\n",
    "        ax.annotate(word, (0, df[dim][i]), color = colors[i], alpha = 0.6, fontsize = 12)\n",
    "    MaxY = df[dim].max()\n",
    "    MinY = df[dim].min()\n",
    "    plt.ylim(MinY,MaxY)\n",
    "    plt.yticks(())\n",
    "    plt.xticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the gender words in the 2 dimensions\n",
    "fig = plt.figure(figsize = (12,4))\n",
    "ax1 = fig.add_subplot(131)\n",
    "PlotDimension(ax1, genderdf_1940_1960, 'nonviolence')\n",
    "ax2 = fig.add_subplot(132)\n",
    "PlotDimension(ax2, genderdf_1940_1960, 'violence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the incentive words in the dimensions \n",
    "fig = plt.figure(figsize = (12,4))\n",
    "ax1 = fig.add_subplot(131)\n",
    "PlotDimension(ax1, incentivedf_1940_1960, 'nonviolence')\n",
    "ax2 = fig.add_subplot(132)\n",
    "PlotDimension(ax2, incentivedf_1940_1960, 'violence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "########## LINGUISTIC CHANGE OR DIFF ##########\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_syn0norm(model):\n",
    "    \"\"\"since syn0norm is now depricated\"\"\"\n",
    "    return (model.wv.syn0 / np.sqrt((model.wv.syn0 ** 2).sum(-1))[..., np.newaxis]).astype(np.float32)\n",
    "\n",
    "def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n",
    "    \"\"\"Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
    "    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "    (With help from William. Thank you!)\n",
    "    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
    "    Then do the alignment on the other_embed model.\n",
    "    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "    Return other_embed.\n",
    "    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
    "    \"\"\"\n",
    "    base_embed = copy.copy(base_embed)\n",
    "    other_embed = copy.copy(other_embed)\n",
    "    # make sure vocabulary and indices are aligned\n",
    "    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
    "\n",
    "    # get the embedding matrices\n",
    "    base_vecs = calc_syn0norm(in_base_embed)\n",
    "    other_vecs = calc_syn0norm(in_other_embed)\n",
    "\n",
    "    # just a matrix dot product with numpy\n",
    "    m = other_vecs.T.dot(base_vecs) \n",
    "    # SVD method from numpy\n",
    "    u, _, v = np.linalg.svd(m)\n",
    "    # another matrix operation\n",
    "    ortho = u.dot(v) \n",
    "    # Replace original array with modified one\n",
    "    # i.e. multiplying the embedding matrix (syn0norm)by \"ortho\"\n",
    "    other_embed.wv.syn0norm = other_embed.wv.syn0 = (calc_syn0norm(other_embed)).dot(ortho)\n",
    "    return other_embed\n",
    "    \n",
    "def intersection_align_gensim(m1,m2, words=None):\n",
    "    \"\"\"\n",
    "    Intersect two gensim word2vec models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.wv.vocab.keys())\n",
    "    vocab_m2 = set(m2.wv.vocab.keys())\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1&vocab_m2\n",
    "    if words: common_vocab&=set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    if not vocab_m1-common_vocab and not vocab_m2-common_vocab:\n",
    "        return (m1,m2)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.wv.vocab[w].count + m2.wv.vocab[w].count,reverse=True)\n",
    "\n",
    "    # Then for each model...\n",
    "    for m in [m1,m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        indices = [m.wv.vocab[w].index for w in common_vocab]\n",
    "        old_arr = calc_syn0norm(m)\n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.wv.syn0norm = m.wv.syn0 = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        m.index2word = common_vocab\n",
    "        old_vocab = m.wv.vocab\n",
    "        new_vocab = {}\n",
    "        for new_index,word in enumerate(common_vocab):\n",
    "            old_vocab_obj=old_vocab[word]\n",
    "            new_vocab[word] = gensim.models.word2vec.Vocab(index=new_index, count=old_vocab_obj.count)\n",
    "        m.wv.vocab = new_vocab\n",
    "\n",
    "    return (m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two collections of embeddings, one the original and one the aligned\n",
    "def compareModels(df, category, text_column_name='normalized_sents', sort = True, embeddings_raw={}):\n",
    "    \"\"\"If you are using time as your category sorting is important\"\"\"\n",
    "    if len(embeddings_raw) == 0:\n",
    "        embeddings_raw = rawModels(df, category, text_column_name, sort)\n",
    "    cats = sorted(set(df[category]))\n",
    "    #These are much quicker\n",
    "    embeddings_aligned = {}\n",
    "    for catOuter in cats:\n",
    "        embeddings_aligned[catOuter] = [embeddings_raw[catOuter]]\n",
    "        for catInner in cats:\n",
    "            embeddings_aligned[catOuter].append(smart_procrustes_align_gensim(embeddings_aligned[catOuter][-1], embeddings_raw[catInner]))\n",
    "    return embeddings_raw, embeddings_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rawModels(df, category, text_column_name='normalized_sents', sort = True):\n",
    "    embeddings_raw = {}\n",
    "    cats = sorted(set(df[category]))\n",
    "    for cat in cats:\n",
    "        #This can take a while\n",
    "        print(\"Embedding {}\".format(cat), end = '\\r')\n",
    "        subsetDF = df[df[category] == cat]\n",
    "        #You might want to change the W2V parameters\n",
    "        embeddings_raw[cat] = gensim.models.word2vec.Word2Vec(subsetDF[text_column_name].sum(), min_count=1)\n",
    "    return embeddings_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 2017\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'copy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-8af6f602ed96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# generate the models:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrawEmbeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomparedEmbeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompareModels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_df_1940_2020\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Year'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-0ffb997de44b>\u001b[0m in \u001b[0;36mcompareModels\u001b[0;34m(df, category, text_column_name, sort, embeddings_raw)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0membeddings_aligned\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcatOuter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0membeddings_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcatOuter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcatInner\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0membeddings_aligned\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcatOuter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmart_procrustes_align_gensim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_aligned\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcatOuter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_raw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcatInner\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0membeddings_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_aligned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-0dfc6858334c>\u001b[0m in \u001b[0;36msmart_procrustes_align_gensim\u001b[0;34m(base_embed, other_embed, words)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintersect\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtwo\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;31m'\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mintersection_align_gensim\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdocumentation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \"\"\"\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mbase_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mother_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# make sure vocabulary and indices are aligned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'copy' is not defined"
     ]
    }
   ],
   "source": [
    "# generate the models:\n",
    "rawEmbeddings, comparedEmbeddings = compareModels(movie_df_1940_2020, 'Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawEmbeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare across all permutions so we will define another function to help\n",
    "def getDivergenceDF(word, embeddingsDict):\n",
    "    dists = []\n",
    "    cats = sorted(set(embeddingsDict.keys()))\n",
    "    dists = {}\n",
    "    print(word)\n",
    "    for cat in cats:\n",
    "        dists[cat] = []\n",
    "        for embed in embeddingsDict[cat][1:]:\n",
    "            dists[cat].append(np.abs(1 - sklearn.metrics.pairwise.cosine_similarity(np.expand_dims(embeddingsDict[cat][0][word], axis = 0),\n",
    "                                                                             np.expand_dims(embed[word], axis = 0))[0,0]))\n",
    "    return pandas.DataFrame(dists, index = cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at some test words\n",
    "## see that the usage of 'right' has stabilized from 1995-now; reason? interesting overlap with digital age\n",
    "## would require more probing\n",
    "## it seems to have undergone a lot of fluctuations, potentially pointing to controversy over its meaning\n",
    "targetWord = 'right'\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the usage of 'nice' has also stabilized since 1995\n",
    "## it seems to have undergone a lot of fluctuations, potentially pointing to controversy over its meaning\n",
    "targetWord = 'nice'\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDiverence(word, embeddingsDict):\n",
    "    cats = sorted(set(embeddingsDict.keys()))\n",
    "    \n",
    "    dists = []\n",
    "    for embed in embeddingsDict[cats[0]][1:]:\n",
    "        dists.append(1 - sklearn.metrics.pairwise.cosine_similarity(np.expand_dims(embeddingsDict[cats[0]][0][word], axis = 0), np.expand_dims(embed[word], axis = 0))[0,0])\n",
    "    return sum(dists)\n",
    "\n",
    "def findMostDivergent(embeddingsDict):\n",
    "    words = []\n",
    "    for embeds in embeddingsDict.values():\n",
    "        for embed in embeds:\n",
    "            words += list(embed.wv.vocab.keys())\n",
    "    words = set(words)\n",
    "    print(\"Found {} words to compare\".format(len(words)))\n",
    "    return sorted([(w, findDiverence(w, embeddingsDict)) for w in words], key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDivergences = findMostDivergent(comparedEmbeddings)\n",
    "wordDivergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the most divergent word 'yes'\n",
    "## note again the 1995 trend\n",
    "## there are certain years that are diff from others -- ~1953,1979,1990\n",
    "targetWord = wordDivergences[0][0]\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'well' is the least divergent word\n",
    "## it also follws the 1995 trend, but has generally been much more stable over time\n",
    "targetWord = wordDivergences[-1][0]\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
