{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Neural Nets and Text\n",
    "## load packages\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers import AdamW, BertForSequenceClassification\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT NOTE\n",
    "# THE ASSOCIATED GOOGLE COLAB FILE LIVES HERE:\n",
    "# https://github.com/lrnbeard/Content-Analysis-2020/blob/master/final_deepLearning_170320.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with loading dataset and pre-processing it. \n",
    "# pre-processing follows similar steps as done in the past, \n",
    "# use pre-written modules offered by the transformers package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lucem_illud_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d010862a95b499abf368538bc35e39d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=230, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Allocate a pipeline for sentiment-analysis\n",
    "nlp_sentiment = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in overall dataframe\n",
    "movie_df = pd.read_csv(\"/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/dataframes/movie_df.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dataframes separated by year\n",
    "movie_df_1940_1960_n75 = pd.read_csv(\"/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/dataframes/movie_df_1940_1960_n75.csv\")\n",
    "movie_df_2000_2020_n75 = pd.read_csv(\"/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/dataframes/movie_df_2000_2020_n75.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Generation using BERT (using the built in generate function)\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model_gpt = AutoModelWithLMHead.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing that we like to do more than analyse data all day long and then try to figure out what's going on.\n",
      "\n",
      "\"We're not going to be able to do that. We're not going to be able to do that.!\n"
     ]
    }
   ],
   "source": [
    "# example generation\n",
    "sequence = \"Nothing that we like to do more than analyse data all day long and\"\n",
    "\n",
    "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_gpt.generate(input, max_length=50)\n",
    "\n",
    "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1940-1960 ###\n",
    "### in these next section sections we will train two models\n",
    "### one to talk like 1940-1960 and the other to talk like 2000-2020\n",
    "### we can then compare how violence is understood between the two\n",
    "### and compare sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_text_1940_1960_n75, test_text_1940_1960_n75 = train_test_split(movie_df_1940_1960_n75['Text'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64    @@4046500 This is Sans Souci . It means \" With...\n",
       "67    @@6333252 Oh , dear , oh , dear . - Good night...\n",
       "61    @@3611996 [?] How many times am I going to tel...\n",
       "63    @@5263092 My name is Allison MacKenzie . Where...\n",
       "25    @@6458656 [Male_Chorus] [Fades] Drums , BugleF...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_1940_1960_n75.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "train_text_1940_1960_n75.to_frame().to_csv(r'/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/output/train_text_1940_1960_n75', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "test_text_1940_1960_n75.to_frame().to_csv(r'/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/output/test_text_1940_1960_n75', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://github.com/lrnbeard/Content-Analysis-2020/blob/master/final_deepLearning_170320.ipynb\n",
    "#tokenizer_1940_1960_n75 = AutoTokenizer.from_pretrained(\"/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/output/output_gpt_1940_1960_n75/\")\n",
    "#model_1940_1960_n75 = AutoModelWithLMHead.from_pretrained(\"/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/output/output_gpt_1940_1960_n75/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://github.com/lrnbeard/Content-Analysis-2020/blob/master/final_deepLearning_170320.ipynb\n",
    "tokenizer_1940_1960_n75 = AutoTokenizer.from_pretrained(\"/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/output/output_1940_FINAL/\")\n",
    "model_1940_1960_n75 = AutoModelWithLMHead.from_pretrained(\"/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/output/output_1940_FINAL/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The definition of violence is to kill. - What's the use of killing? - I don't know. I don't know what it means to kill. - What's the use of killing?\"\n"
     ]
    }
   ],
   "source": [
    "# here we start text generation -- see violence is bad, guns are not allowed, etc\n",
    "sequence = \"The definition of violence is\"\n",
    "\n",
    "input = tokenizer_1940_1960_n75.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_1940_1960_n75.generate(input, max_length=41, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_1940_1960_n75.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guns are not allowed in the house. - No, no, no.\"\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Guns are\"\n",
    "\n",
    "input = tokenizer_1940_1960_n75.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_1940_1960_n75.generate(input, max_length=17, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_1940_1960_n75.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuck who? - I'm sorry.\"\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Fuck who?\"\n",
    "\n",
    "input = tokenizer_1940_1960_n75.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_1940_1960_n75.generate(input, max_length=9, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_1940_1960_n75.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that all of these sentiment scores are very negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9986436}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"The definition of violence is to kill. - What's the use of killing? - I don't know. I don't know what it means to kill.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.99901944}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"Guns are not allowed in the house. - No, no, no.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.999546}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"Fuck who? - I'm sorry.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2000-2020 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_text_2000_2020_n75, test_text_2000_2020_n75 = train_test_split(movie_df_2000_2020_n75['Text'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29    @@6093769 What's his name ? - Bear . - Are you...\n",
       "7     @@3313049 Good for you , Pete . Good for you ....\n",
       "17    @@5867452 ( @baby_cries ) ( @baby_cries ) ( CA...\n",
       "9     @@6759672 Do you think you can know something ...\n",
       "48    @@4553869 Turn over to the right . You're squi...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_2000_2020_n75.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_2000_2020_n75.to_frame().to_csv(r'/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/output/train_text_2000_2020_n75', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_2000_2020_n75.to_frame().to_csv(r'/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/output/test_text_2000_2020_n75', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://github.com/lrnbeard/Content-Analysis-2020/blob/master/final_deepLearning_170320.ipynb\n",
    "#tokenizer_2000_2020_n75 = AutoTokenizer.from_pretrained(\"/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/output/output_2000_2020_n75/\")\n",
    "#model_2000_2020_n75 = AutoModelWithLMHead.from_pretrained(\"/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/output/output_2000_2020_n75\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://github.com/lrnbeard/Content-Analysis-2020/blob/master/final_deepLearning_170320.ipynb\n",
    "tokenizer_2000_2020_n75 = AutoTokenizer.from_pretrained(\"/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/output/output_2000_FINAL/\")\n",
    "model_2000_2020_n75 = AutoModelWithLMHead.from_pretrained(\"/Users/laurenbeard/Desktop/compContentAnalysis/Content-Analysis-2020/final/output/output_2000_FINAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The definition of violence is that it's a way of life\"\n"
     ]
    }
   ],
   "source": [
    "# violence is much more liberally spoken about here!\n",
    "sequence = \"The definition of violence is\"\n",
    "\n",
    "input = tokenizer_2000_2020_n75.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_2000_2020_n75.generate(input, max_length=13, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_2000_2020_n75.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guns are not the only thing that makes you feel good.\"\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Guns are\"\n",
    "\n",
    "input = tokenizer_2000_2020_n75.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_2000_2020_n75.generate(input, max_length=14, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_2000_2020_n75.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuck who? - I'm not gon na be able to find you.\"\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Fuck who?\"\n",
    "\n",
    "input = tokenizer_2000_2020_n75.encode(sequence, return_tensors=\"pt\")\n",
    "generated = model_2000_2020_n75.generate(input, max_length=16, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
    "\n",
    "resulting_string = tokenizer_2000_2020_n75.decode(generated.tolist()[0])\n",
    "print(resulting_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that not only was violence spoken about more liberally \n",
    "# but the following sentiment scores are more negative (except for the last)\n",
    "# but the context is still quite diff from the 1940-1960 example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.70927584}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"The definition of violence is that it's a way of life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.99196535}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"Guns are not the only thing that makes you feel good.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.99931544}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_sentiment(\"Fuck who? - I'm not gon na be able to find you.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
