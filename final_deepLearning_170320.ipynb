{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_deepLearning_170320.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNmAx3aBiWhxQ3hxXvK35DT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lrnbeard/Content-Analysis-2020/blob/master/final_deepLearning_170320.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWuJWbqhgyPe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "ee346727-902a-4667-a3ac-43cb2d16e8a8"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MjgB2S-g4iD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d17f4e7a-3913-4a83-c099-2aaadb7d1bd8"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eodC14zmg4nd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "55ae7b89-13b0-4e51-836c-0055880f0be2"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.18)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.18 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.18)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.18->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.18->boto3->transformers) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZNlHcG2g4lB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers import AdamW, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvpBGr3fg9rp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d464ab0c-ae36-496e-c166-3edc772b86bf"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvJOLZeGg9v_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wget\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUqYjtmig91F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5B41wqVhL2F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "5cf4565f-f1dc-44b1-9719-32a55a8382b2"
      },
      "source": [
        "!python /content/run_language_modelling.py --output_dir=output_gpt_1940_1960 --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=/content/train_text_1940_1960 --do_eval --eval_data_file=/content/test_text_1940_1960 --per_gpu_train_batch_size=1 --per_gpu_eval_batch_size=1"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/run_language_modelling.py\", line 799, in <module>\n",
            "    main()\n",
            "  File \"/content/run_language_modelling.py\", line 650, in main\n",
            "    args.output_dir\n",
            "ValueError: Output directory (output_gpt_1940_1960) already exists and is not empty. Use --overwrite_output_dir to overcome.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2JV6qkwhNgO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c855b2ee-25ae-4484-da56-98c67ae97837"
      },
      "source": [
        "!python run_generation.py --model_type=gpt2 --model_name_or_path=/content/output_gpt_1940_1960"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/19/2020 16:39:49 - INFO - transformers.tokenization_utils -   Model name '/content/output_gpt_1940_1960' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/output_gpt_1940_1960' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/19/2020 16:39:49 - INFO - transformers.tokenization_utils -   Didn't find file /content/output_gpt_1940_1960/added_tokens.json. We won't load it.\n",
            "03/19/2020 16:39:49 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_1940_1960/vocab.json\n",
            "03/19/2020 16:39:49 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_1940_1960/merges.txt\n",
            "03/19/2020 16:39:49 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/19/2020 16:39:49 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_1940_1960/special_tokens_map.json\n",
            "03/19/2020 16:39:49 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_1940_1960/tokenizer_config.json\n",
            "03/19/2020 16:39:49 - INFO - transformers.configuration_utils -   loading configuration file /content/output_gpt_1940_1960/config.json\n",
            "03/19/2020 16:39:49 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "03/19/2020 16:39:49 - INFO - transformers.modeling_utils -   loading weights file /content/output_gpt_1940_1960/pytorch_model.bin\n",
            "03/19/2020 16:39:56 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=0, length=20, model_name_or_path='/content/output_gpt_1940_1960', model_type='gpt2', n_gpu=1, no_cuda=False, num_return_sequences=1, p=0.9, padding_text='', prompt='', repetition_penalty=1.0, seed=42, stop_token=None, temperature=1.0, xlm_language='')\n",
            "Model prompt >>> Traceback (most recent call last):\n",
            "  File \"run_generation.py\", line 263, in <module>\n",
            "  File \"run_generation.py\", line 210, in main\n",
            "    prompt_text = args.prompt if args.prompt else input(\"Model prompt >>> \")\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zkBvmZthahV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "6c1940af-cd48-4d38-9949-456a2d2fc555"
      },
      "source": [
        "!python /content/run_language_modelling.py --output_dir=output_gpt_2000_2020 --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=/content/train_text_2000_2020 --do_eval --eval_data_file=/content/test_text_2000_2020 --per_gpu_train_batch_size=1 --per_gpu_eval_batch_size=1"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/run_language_modelling.py\", line 40, in <module>\n",
            "    from transformers import (\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/__init__.py\", line 22, in <module>\n",
            "    from .configuration_albert import ALBERT_PRETRAINED_CONFIG_ARCHIVE_MAP, AlbertConfig\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/configuration_albert.py\", line 18, in <module>\n",
            "    from .configuration_utils import PretrainedConfig\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py\", line 25, in <module>\n",
            "    from .file_utils import CONFIG_NAME, cached_path, hf_bucket_url, is_remote_url\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/file_utils.py\", line 53, in <module>\n",
            "    import tensorflow as tf\n",
            "  File \"/tensorflow-1.15.0/python3.6/tensorflow/__init__.py\", line 99, in <module>\n",
            "    from tensorflow_core import *\n",
            "  File \"/tensorflow-1.15.0/python3.6/tensorflow_core/__init__.py\", line 28, in <module>\n",
            "    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1007, in _handle_fromlist\n",
            "  File \"/tensorflow-1.15.0/python3.6/tensorflow/__init__.py\", line 50, in __getattr__\n",
            "    module = self._load()\n",
            "  File \"/tensorflow-1.15.0/python3.6/tensorflow/__init__.py\", line 44, in _load\n",
            "    module = _importlib.import_module(self.__name__)\n",
            "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/tensorflow-1.15.0/python3.6/tensorflow_core/python/__init__.py\", line 63, in <module>\n",
            "    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin\n",
            "  File \"/tensorflow-1.15.0/python3.6/tensorflow_core/python/framework/framework_lib.py\", line 52, in <module>\n",
            "    from tensorflow.python.framework.importer import import_graph_def\n",
            "  File \"/tensorflow-1.15.0/python3.6/tensorflow_core/python/framework/importer.py\", line 28, in <module>\n",
            "    from tensorflow.python.framework import function\n",
            "  File \"/tensorflow-1.15.0/python3.6/tensorflow_core/python/framework/function.py\", line 38, in <module>\n",
            "    from tensorflow.python.ops import variable_scope as vs\n",
            "  File \"/tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/variable_scope.py\", line 40, in <module>\n",
            "    from tensorflow.python.ops import init_ops\n",
            "  File \"/tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/init_ops.py\", line 44, in <module>\n",
            "    from tensorflow.python.ops import gen_linalg_ops\n",
            "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 674, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 779, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 487, in _compile_bytecode\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4BbARzThbSD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "52843379-1697-41a8-81c0-6a7b9f4d43f7"
      },
      "source": [
        "!python run_generation.py --model_type=gpt2 --model_name_or_path=/content/output_gpt_2000_2020"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/19/2020 16:40:07 - INFO - transformers.tokenization_utils -   Model name '/content/output_gpt_2000_2020' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/output_gpt_2000_2020' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/19/2020 16:40:07 - INFO - transformers.tokenization_utils -   Didn't find file /content/output_gpt_2000_2020/added_tokens.json. We won't load it.\n",
            "03/19/2020 16:40:07 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_2000_2020/vocab.json\n",
            "03/19/2020 16:40:07 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_2000_2020/merges.txt\n",
            "03/19/2020 16:40:07 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/19/2020 16:40:07 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_2000_2020/special_tokens_map.json\n",
            "03/19/2020 16:40:07 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_2000_2020/tokenizer_config.json\n",
            "03/19/2020 16:40:07 - INFO - transformers.configuration_utils -   loading configuration file /content/output_gpt_2000_2020/config.json\n",
            "03/19/2020 16:40:07 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "03/19/2020 16:40:07 - INFO - transformers.modeling_utils -   loading weights file /content/output_gpt_2000_2020/pytorch_model.bin\n",
            "Traceback (most recent call last):\n",
            "  File \"run_generation.py\", line 263, in <module>\n",
            "    main()\n",
            "  File \"run_generation.py\", line 204, in main\n",
            "    model = model_class.from_pretrained(args.model_name_or_path)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py\", line 466, in from_pretrained\n",
            "    model = cls(config, *model_args, **model_kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\", line 514, in __init__\n",
            "    self.transformer = GPT2Model(config)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\", line 333, in __init__\n",
            "    self.h = nn.ModuleList([Block(config.n_ctx, config, scale=True) for _ in range(config.n_layer)])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\", line 333, in <listcomp>\n",
            "    self.h = nn.ModuleList([Block(config.n_ctx, config, scale=True) for _ in range(config.n_layer)])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\", line 222, in __init__\n",
            "    self.mlp = MLP(4 * nx, config)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\", line 205, in __init__\n",
            "    self.c_proj = Conv1D(nx, n_state)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py\", line 1188, in __init__\n",
            "    nn.init.normal_(w, std=0.02)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/init.py\", line 105, in normal_\n",
            "    return _no_grad_normal_(tensor, mean, std)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/init.py\", line 19, in _no_grad_normal_\n",
            "    return tensor.normal_(mean, std)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}