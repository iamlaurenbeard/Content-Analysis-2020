{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_deepLearning_170320.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNMPHc/fPV/cInGZm2FFOaq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lrnbeard/Content-Analysis-2020/blob/master/final_deepLearning_170320.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWuJWbqhgyPe",
        "colab_type": "code",
        "outputId": "1e028bde-911f-4ed1-a542-ae84d4beba48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MjgB2S-g4iD",
        "colab_type": "code",
        "outputId": "e4da147f-6e05-4e3d-b5c4-99e1c1c72bff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eodC14zmg4nd",
        "colab_type": "code",
        "outputId": "20222d05-8e27-41df-94ad-da5c633bd806",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/33/ffb67897a6985a7b7d8e5e7878c3628678f553634bd3836404fef06ef19b/transformers-2.5.1-py3-none-any.whl (499kB)\n",
            "\r\u001b[K     |▋                               | 10kB 22.9MB/s eta 0:00:01\r\u001b[K     |█▎                              | 20kB 25.2MB/s eta 0:00:01\r\u001b[K     |██                              | 30kB 30.0MB/s eta 0:00:01\r\u001b[K     |██▋                             | 40kB 33.0MB/s eta 0:00:01\r\u001b[K     |███▎                            | 51kB 35.6MB/s eta 0:00:01\r\u001b[K     |████                            | 61kB 37.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 71kB 38.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 81kB 38.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 92kB 40.1MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 102kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 112kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 122kB 41.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 133kB 41.7MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 143kB 41.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 153kB 41.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 163kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 174kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 184kB 41.7MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 194kB 41.7MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 204kB 41.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 215kB 41.7MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 225kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 235kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 245kB 41.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 256kB 41.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 266kB 41.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 276kB 41.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 286kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 296kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 307kB 41.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 317kB 41.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 327kB 41.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 337kB 41.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 348kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 358kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 368kB 41.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 378kB 41.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 389kB 41.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 399kB 41.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 409kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 419kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 430kB 41.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 440kB 41.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 450kB 41.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 460kB 41.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 471kB 41.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 481kB 41.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 491kB 41.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 501kB 41.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 65.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.23)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 37.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 60.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.23 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.23)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=2bb6036f1d23e39a09dcab7d2c2661080d3bd07d75ded434c308bf46e6c33d0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZNlHcG2g4lB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers import AdamW, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvpBGr3fg9rp",
        "colab_type": "code",
        "outputId": "8325c119-a978-4812-c374-18948fdbfe56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=8bd28216da2490d309c391ff97ff1f3f6fa4a0a25d31f343766d3a838321b100\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvJOLZeGg9v_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wget\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUqYjtmig91F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5B41wqVhL2F",
        "colab_type": "code",
        "outputId": "e3b28334-6d26-48fb-cc91-dcf9b1a58cdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python /content/run_language_modelling.py --output_dir=output_gpt_1940_1960_n75 --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=/content/train_text_1940_1960_n75 --do_eval --eval_data_file=/content/test_text_1940_1960_n75 --per_gpu_train_batch_size=1 --per_gpu_eval_batch_size=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/21/2020 22:02:31 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "03/21/2020 22:02:31 - INFO - filelock -   Lock 140546072213544 acquired on /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942.lock\n",
            "03/21/2020 22:02:31 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp8458gjzq\n",
            "Downloading: 100% 224/224 [00:00<00:00, 222kB/s]\n",
            "03/21/2020 22:02:32 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json in cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n",
            "03/21/2020 22:02:32 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n",
            "03/21/2020 22:02:32 - INFO - filelock -   Lock 140546072213544 released on /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942.lock\n",
            "03/21/2020 22:02:32 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n",
            "03/21/2020 22:02:32 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "03/21/2020 22:02:32 - INFO - filelock -   Lock 140546072213544 acquired on /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock\n",
            "03/21/2020 22:02:32 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpo2pa7elk\n",
            "Downloading: 100% 1.04M/1.04M [00:00<00:00, 1.89MB/s]\n",
            "03/21/2020 22:02:33 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json in cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "03/21/2020 22:02:33 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "03/21/2020 22:02:33 - INFO - filelock -   Lock 140546072213544 released on /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock\n",
            "03/21/2020 22:02:33 - INFO - filelock -   Lock 140545699413464 acquired on /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "03/21/2020 22:02:33 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpgkwsk8hb\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 746kB/s]\n",
            "03/21/2020 22:02:35 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt in cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/21/2020 22:02:35 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/21/2020 22:02:35 - INFO - filelock -   Lock 140545699413464 released on /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "03/21/2020 22:02:35 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "03/21/2020 22:02:35 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/21/2020 22:02:35 - INFO - filelock -   Lock 140545699410552 acquired on /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1.lock\n",
            "03/21/2020 22:02:35 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpsev7t_xd\n",
            "Downloading: 100% 548M/548M [00:21<00:00, 25.0MB/s]\n",
            "03/21/2020 22:02:58 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin in cache at /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "03/21/2020 22:02:58 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "03/21/2020 22:02:58 - INFO - filelock -   Lock 140545699410552 released on /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1.lock\n",
            "03/21/2020 22:02:58 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "03/21/2020 22:03:11 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=1024, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='/content/test_text_1940_1960_n75', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='gpt2', model_type='gpt2', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='output_gpt_1940_1960_n75', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=1, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='/content/train_text_1940_1960_n75', warmup_steps=0, weight_decay=0.0)\n",
            "03/21/2020 22:03:11 - INFO - __main__ -   Creating features from dataset file at /content\n",
            "03/21/2020 22:03:13 - INFO - __main__ -   Saving features into cached file /content/gpt2_cached_lm_1024_train_text_1940_1960_n75\n",
            "03/21/2020 22:03:13 - INFO - __main__ -   ***** Running training *****\n",
            "03/21/2020 22:03:13 - INFO - __main__ -     Num examples = 449\n",
            "03/21/2020 22:03:13 - INFO - __main__ -     Num Epochs = 1\n",
            "03/21/2020 22:03:13 - INFO - __main__ -     Instantaneous batch size per GPU = 1\n",
            "03/21/2020 22:03:13 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "03/21/2020 22:03:13 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "03/21/2020 22:03:13 - INFO - __main__ -     Total optimization steps = 449\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/449 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/449 [00:00<04:51,  1.54it/s]\u001b[A\n",
            "Iteration:   0% 2/449 [00:01<04:12,  1.77it/s]\u001b[A\n",
            "Iteration:   1% 3/449 [00:01<03:47,  1.96it/s]\u001b[A\n",
            "Iteration:   1% 4/449 [00:01<03:31,  2.11it/s]\u001b[A\n",
            "Iteration:   1% 5/449 [00:02<03:17,  2.24it/s]\u001b[A\n",
            "Iteration:   1% 6/449 [00:02<03:08,  2.35it/s]\u001b[A\n",
            "Iteration:   2% 7/449 [00:02<03:02,  2.42it/s]\u001b[A\n",
            "Iteration:   2% 8/449 [00:03<02:58,  2.47it/s]\u001b[A\n",
            "Iteration:   2% 9/449 [00:03<02:54,  2.52it/s]\u001b[A\n",
            "Iteration:   2% 10/449 [00:04<02:52,  2.55it/s]\u001b[A\n",
            "Iteration:   2% 11/449 [00:04<02:50,  2.57it/s]\u001b[A\n",
            "Iteration:   3% 12/449 [00:04<02:49,  2.58it/s]\u001b[A\n",
            "Iteration:   3% 13/449 [00:05<02:48,  2.59it/s]\u001b[A\n",
            "Iteration:   3% 14/449 [00:05<02:47,  2.59it/s]\u001b[A\n",
            "Iteration:   3% 15/449 [00:05<02:47,  2.59it/s]\u001b[A\n",
            "Iteration:   4% 16/449 [00:06<02:46,  2.60it/s]\u001b[A\n",
            "Iteration:   4% 17/449 [00:06<02:46,  2.59it/s]\u001b[A\n",
            "Iteration:   4% 18/449 [00:07<02:46,  2.59it/s]\u001b[A\n",
            "Iteration:   4% 19/449 [00:07<02:45,  2.60it/s]\u001b[A\n",
            "Iteration:   4% 20/449 [00:07<02:45,  2.60it/s]\u001b[A\n",
            "Iteration:   5% 21/449 [00:08<02:44,  2.60it/s]\u001b[A\n",
            "Iteration:   5% 22/449 [00:08<02:44,  2.59it/s]\u001b[A\n",
            "Iteration:   5% 23/449 [00:09<02:44,  2.59it/s]\u001b[A\n",
            "Iteration:   5% 24/449 [00:09<02:43,  2.60it/s]\u001b[A\n",
            "Iteration:   6% 25/449 [00:09<02:43,  2.60it/s]\u001b[A\n",
            "Iteration:   6% 26/449 [00:10<02:42,  2.60it/s]\u001b[A\n",
            "Iteration:   6% 27/449 [00:10<02:42,  2.60it/s]\u001b[A\n",
            "Iteration:   6% 28/449 [00:11<02:41,  2.60it/s]\u001b[A\n",
            "Iteration:   6% 29/449 [00:11<02:41,  2.60it/s]\u001b[A\n",
            "Iteration:   7% 30/449 [00:11<02:41,  2.60it/s]\u001b[A\n",
            "Iteration:   7% 31/449 [00:12<02:40,  2.60it/s]\u001b[A\n",
            "Iteration:   7% 32/449 [00:12<02:40,  2.60it/s]\u001b[A\n",
            "Iteration:   7% 33/449 [00:12<02:40,  2.60it/s]\u001b[A\n",
            "Iteration:   8% 34/449 [00:13<02:39,  2.60it/s]\u001b[A\n",
            "Iteration:   8% 35/449 [00:13<02:39,  2.60it/s]\u001b[A\n",
            "Iteration:   8% 36/449 [00:14<02:39,  2.60it/s]\u001b[A\n",
            "Iteration:   8% 37/449 [00:14<02:38,  2.59it/s]\u001b[A\n",
            "Iteration:   8% 38/449 [00:14<02:38,  2.59it/s]\u001b[A\n",
            "Iteration:   9% 39/449 [00:15<02:38,  2.59it/s]\u001b[A\n",
            "Iteration:   9% 40/449 [00:15<02:38,  2.58it/s]\u001b[A\n",
            "Iteration:   9% 41/449 [00:16<02:38,  2.58it/s]\u001b[A\n",
            "Iteration:   9% 42/449 [00:16<02:37,  2.58it/s]\u001b[A\n",
            "Iteration:  10% 43/449 [00:16<02:37,  2.58it/s]\u001b[A\n",
            "Iteration:  10% 44/449 [00:17<02:36,  2.58it/s]\u001b[A\n",
            "Iteration:  10% 45/449 [00:17<02:36,  2.58it/s]\u001b[A\n",
            "Iteration:  10% 46/449 [00:17<02:35,  2.58it/s]\u001b[A\n",
            "Iteration:  10% 47/449 [00:18<02:35,  2.58it/s]\u001b[A\n",
            "Iteration:  11% 48/449 [00:18<02:35,  2.58it/s]\u001b[A\n",
            "Iteration:  11% 49/449 [00:19<02:34,  2.58it/s]\u001b[A\n",
            "Iteration:  11% 50/449 [00:19<02:34,  2.58it/s]\u001b[A\n",
            "Iteration:  11% 51/449 [00:19<02:34,  2.58it/s]\u001b[A\n",
            "Iteration:  12% 52/449 [00:20<02:33,  2.58it/s]\u001b[A\n",
            "Iteration:  12% 53/449 [00:20<02:33,  2.58it/s]\u001b[A\n",
            "Iteration:  12% 54/449 [00:21<02:33,  2.58it/s]\u001b[A\n",
            "Iteration:  12% 55/449 [00:21<02:32,  2.58it/s]\u001b[A\n",
            "Iteration:  12% 56/449 [00:21<02:32,  2.58it/s]\u001b[A\n",
            "Iteration:  13% 57/449 [00:22<02:31,  2.58it/s]\u001b[A\n",
            "Iteration:  13% 58/449 [00:22<02:31,  2.58it/s]\u001b[A\n",
            "Iteration:  13% 59/449 [00:22<02:30,  2.59it/s]\u001b[A\n",
            "Iteration:  13% 60/449 [00:23<02:31,  2.58it/s]\u001b[A\n",
            "Iteration:  14% 61/449 [00:23<02:30,  2.57it/s]\u001b[A\n",
            "Iteration:  14% 62/449 [00:24<02:29,  2.58it/s]\u001b[A\n",
            "Iteration:  14% 63/449 [00:24<02:30,  2.57it/s]\u001b[A\n",
            "Iteration:  14% 64/449 [00:24<02:29,  2.57it/s]\u001b[A\n",
            "Iteration:  14% 65/449 [00:25<02:29,  2.58it/s]\u001b[A\n",
            "Iteration:  15% 66/449 [00:25<02:28,  2.57it/s]\u001b[A\n",
            "Iteration:  15% 67/449 [00:26<02:28,  2.57it/s]\u001b[A\n",
            "Iteration:  15% 68/449 [00:26<02:28,  2.57it/s]\u001b[A\n",
            "Iteration:  15% 69/449 [00:26<02:28,  2.56it/s]\u001b[A\n",
            "Iteration:  16% 70/449 [00:27<02:27,  2.56it/s]\u001b[A\n",
            "Iteration:  16% 71/449 [00:27<02:27,  2.57it/s]\u001b[A\n",
            "Iteration:  16% 72/449 [00:28<02:27,  2.56it/s]\u001b[A\n",
            "Iteration:  16% 73/449 [00:28<02:26,  2.56it/s]\u001b[A\n",
            "Iteration:  16% 74/449 [00:28<02:26,  2.56it/s]\u001b[A\n",
            "Iteration:  17% 75/449 [00:29<02:26,  2.56it/s]\u001b[A\n",
            "Iteration:  17% 76/449 [00:29<02:25,  2.56it/s]\u001b[A\n",
            "Iteration:  17% 77/449 [00:30<02:25,  2.56it/s]\u001b[A\n",
            "Iteration:  17% 78/449 [00:30<02:24,  2.56it/s]\u001b[A\n",
            "Iteration:  18% 79/449 [00:30<02:24,  2.56it/s]\u001b[A\n",
            "Iteration:  18% 80/449 [00:31<02:23,  2.57it/s]\u001b[A\n",
            "Iteration:  18% 81/449 [00:31<02:23,  2.56it/s]\u001b[A\n",
            "Iteration:  18% 82/449 [00:31<02:23,  2.55it/s]\u001b[A\n",
            "Iteration:  18% 83/449 [00:32<02:23,  2.56it/s]\u001b[A\n",
            "Iteration:  19% 84/449 [00:32<02:22,  2.56it/s]\u001b[A\n",
            "Iteration:  19% 85/449 [00:33<02:22,  2.56it/s]\u001b[A\n",
            "Iteration:  19% 86/449 [00:33<02:22,  2.56it/s]\u001b[A\n",
            "Iteration:  19% 87/449 [00:33<02:21,  2.56it/s]\u001b[A\n",
            "Iteration:  20% 88/449 [00:34<02:21,  2.56it/s]\u001b[A\n",
            "Iteration:  20% 89/449 [00:34<02:21,  2.55it/s]\u001b[A\n",
            "Iteration:  20% 90/449 [00:35<02:20,  2.56it/s]\u001b[A\n",
            "Iteration:  20% 91/449 [00:35<02:19,  2.56it/s]\u001b[A\n",
            "Iteration:  20% 92/449 [00:35<02:20,  2.55it/s]\u001b[A\n",
            "Iteration:  21% 93/449 [00:36<02:19,  2.55it/s]\u001b[A\n",
            "Iteration:  21% 94/449 [00:36<02:19,  2.55it/s]\u001b[A\n",
            "Iteration:  21% 95/449 [00:37<02:18,  2.55it/s]\u001b[A\n",
            "Iteration:  21% 96/449 [00:37<02:18,  2.55it/s]\u001b[A\n",
            "Iteration:  22% 97/449 [00:37<02:17,  2.55it/s]\u001b[A\n",
            "Iteration:  22% 98/449 [00:38<02:17,  2.55it/s]\u001b[A\n",
            "Iteration:  22% 99/449 [00:38<02:17,  2.55it/s]\u001b[A\n",
            "Iteration:  22% 100/449 [00:39<02:16,  2.55it/s]\u001b[A\n",
            "Iteration:  22% 101/449 [00:39<02:16,  2.55it/s]\u001b[A\n",
            "Iteration:  23% 102/449 [00:39<02:16,  2.55it/s]\u001b[A\n",
            "Iteration:  23% 103/449 [00:40<02:15,  2.55it/s]\u001b[A\n",
            "Iteration:  23% 104/449 [00:40<02:15,  2.55it/s]\u001b[A\n",
            "Iteration:  23% 105/449 [00:40<02:15,  2.55it/s]\u001b[A\n",
            "Iteration:  24% 106/449 [00:41<02:14,  2.55it/s]\u001b[A\n",
            "Iteration:  24% 107/449 [00:41<02:14,  2.55it/s]\u001b[A\n",
            "Iteration:  24% 108/449 [00:42<02:14,  2.54it/s]\u001b[A\n",
            "Iteration:  24% 109/449 [00:42<02:13,  2.55it/s]\u001b[A\n",
            "Iteration:  24% 110/449 [00:42<02:13,  2.54it/s]\u001b[A\n",
            "Iteration:  25% 111/449 [00:43<02:12,  2.54it/s]\u001b[A\n",
            "Iteration:  25% 112/449 [00:43<02:12,  2.54it/s]\u001b[A\n",
            "Iteration:  25% 113/449 [00:44<02:12,  2.54it/s]\u001b[A\n",
            "Iteration:  25% 114/449 [00:44<02:11,  2.54it/s]\u001b[A\n",
            "Iteration:  26% 115/449 [00:44<02:11,  2.54it/s]\u001b[A\n",
            "Iteration:  26% 116/449 [00:45<02:11,  2.54it/s]\u001b[A\n",
            "Iteration:  26% 117/449 [00:45<02:11,  2.53it/s]\u001b[A\n",
            "Iteration:  26% 118/449 [00:46<02:10,  2.54it/s]\u001b[A\n",
            "Iteration:  27% 119/449 [00:46<02:10,  2.53it/s]\u001b[A\n",
            "Iteration:  27% 120/449 [00:46<02:10,  2.53it/s]\u001b[A\n",
            "Iteration:  27% 121/449 [00:47<02:09,  2.53it/s]\u001b[A\n",
            "Iteration:  27% 122/449 [00:47<02:09,  2.53it/s]\u001b[A\n",
            "Iteration:  27% 123/449 [00:48<02:08,  2.53it/s]\u001b[A\n",
            "Iteration:  28% 124/449 [00:48<02:08,  2.53it/s]\u001b[A\n",
            "Iteration:  28% 125/449 [00:48<02:08,  2.53it/s]\u001b[A\n",
            "Iteration:  28% 126/449 [00:49<02:07,  2.53it/s]\u001b[A\n",
            "Iteration:  28% 127/449 [00:49<02:07,  2.53it/s]\u001b[A\n",
            "Iteration:  29% 128/449 [00:50<02:06,  2.53it/s]\u001b[A\n",
            "Iteration:  29% 129/449 [00:50<02:06,  2.53it/s]\u001b[A\n",
            "Iteration:  29% 130/449 [00:50<02:06,  2.53it/s]\u001b[A\n",
            "Iteration:  29% 131/449 [00:51<02:05,  2.53it/s]\u001b[A\n",
            "Iteration:  29% 132/449 [00:51<02:05,  2.53it/s]\u001b[A\n",
            "Iteration:  30% 133/449 [00:52<02:04,  2.53it/s]\u001b[A\n",
            "Iteration:  30% 134/449 [00:52<02:04,  2.53it/s]\u001b[A\n",
            "Iteration:  30% 135/449 [00:52<02:03,  2.53it/s]\u001b[A\n",
            "Iteration:  30% 136/449 [00:53<02:03,  2.53it/s]\u001b[A\n",
            "Iteration:  31% 137/449 [00:53<02:03,  2.53it/s]\u001b[A\n",
            "Iteration:  31% 138/449 [00:54<02:06,  2.47it/s]\u001b[A\n",
            "Iteration:  31% 139/449 [00:54<02:04,  2.49it/s]\u001b[A\n",
            "Iteration:  31% 140/449 [00:54<02:04,  2.48it/s]\u001b[A\n",
            "Iteration:  31% 141/449 [00:55<02:03,  2.50it/s]\u001b[A\n",
            "Iteration:  32% 142/449 [00:55<02:02,  2.51it/s]\u001b[A\n",
            "Iteration:  32% 143/449 [00:56<02:01,  2.51it/s]\u001b[A\n",
            "Iteration:  32% 144/449 [00:56<02:01,  2.52it/s]\u001b[A\n",
            "Iteration:  32% 145/449 [00:56<02:00,  2.52it/s]\u001b[A\n",
            "Iteration:  33% 146/449 [00:57<02:00,  2.52it/s]\u001b[A\n",
            "Iteration:  33% 147/449 [00:57<01:59,  2.52it/s]\u001b[A\n",
            "Iteration:  33% 148/449 [00:58<01:59,  2.52it/s]\u001b[A\n",
            "Iteration:  33% 149/449 [00:58<01:59,  2.52it/s]\u001b[A\n",
            "Iteration:  33% 150/449 [00:58<01:58,  2.52it/s]\u001b[A\n",
            "Iteration:  34% 151/449 [00:59<01:58,  2.52it/s]\u001b[A\n",
            "Iteration:  34% 152/449 [00:59<01:58,  2.51it/s]\u001b[A\n",
            "Iteration:  34% 153/449 [00:59<01:57,  2.52it/s]\u001b[A\n",
            "Iteration:  34% 154/449 [01:00<01:57,  2.51it/s]\u001b[A\n",
            "Iteration:  35% 155/449 [01:00<01:57,  2.51it/s]\u001b[A\n",
            "Iteration:  35% 156/449 [01:01<01:57,  2.50it/s]\u001b[A\n",
            "Iteration:  35% 157/449 [01:01<01:56,  2.51it/s]\u001b[A\n",
            "Iteration:  35% 158/449 [01:01<01:55,  2.51it/s]\u001b[A\n",
            "Iteration:  35% 159/449 [01:02<01:55,  2.51it/s]\u001b[A\n",
            "Iteration:  36% 160/449 [01:02<01:55,  2.51it/s]\u001b[A\n",
            "Iteration:  36% 161/449 [01:03<01:54,  2.51it/s]\u001b[A\n",
            "Iteration:  36% 162/449 [01:03<01:54,  2.51it/s]\u001b[A\n",
            "Iteration:  36% 163/449 [01:03<01:54,  2.51it/s]\u001b[A\n",
            "Iteration:  37% 164/449 [01:04<01:53,  2.50it/s]\u001b[A\n",
            "Iteration:  37% 165/449 [01:04<01:53,  2.51it/s]\u001b[A\n",
            "Iteration:  37% 166/449 [01:05<01:52,  2.51it/s]\u001b[A\n",
            "Iteration:  37% 167/449 [01:05<01:52,  2.50it/s]\u001b[A\n",
            "Iteration:  37% 168/449 [01:05<01:52,  2.50it/s]\u001b[A\n",
            "Iteration:  38% 169/449 [01:06<01:51,  2.50it/s]\u001b[A\n",
            "Iteration:  38% 170/449 [01:06<01:51,  2.50it/s]\u001b[A\n",
            "Iteration:  38% 171/449 [01:07<01:51,  2.50it/s]\u001b[A\n",
            "Iteration:  38% 172/449 [01:07<01:50,  2.50it/s]\u001b[A\n",
            "Iteration:  39% 173/449 [01:07<01:50,  2.50it/s]\u001b[A\n",
            "Iteration:  39% 174/449 [01:08<01:50,  2.50it/s]\u001b[A\n",
            "Iteration:  39% 175/449 [01:08<01:49,  2.50it/s]\u001b[A\n",
            "Iteration:  39% 176/449 [01:09<01:49,  2.50it/s]\u001b[A\n",
            "Iteration:  39% 177/449 [01:09<01:48,  2.50it/s]\u001b[A\n",
            "Iteration:  40% 178/449 [01:09<01:48,  2.50it/s]\u001b[A\n",
            "Iteration:  40% 179/449 [01:10<01:48,  2.50it/s]\u001b[A\n",
            "Iteration:  40% 180/449 [01:10<01:47,  2.50it/s]\u001b[A\n",
            "Iteration:  40% 181/449 [01:11<01:47,  2.49it/s]\u001b[A\n",
            "Iteration:  41% 182/449 [01:11<01:46,  2.50it/s]\u001b[A\n",
            "Iteration:  41% 183/449 [01:11<01:46,  2.50it/s]\u001b[A\n",
            "Iteration:  41% 184/449 [01:12<01:46,  2.49it/s]\u001b[A\n",
            "Iteration:  41% 185/449 [01:12<01:46,  2.49it/s]\u001b[A\n",
            "Iteration:  41% 186/449 [01:13<01:45,  2.49it/s]\u001b[A\n",
            "Iteration:  42% 187/449 [01:13<01:45,  2.49it/s]\u001b[A\n",
            "Iteration:  42% 188/449 [01:13<01:44,  2.49it/s]\u001b[A\n",
            "Iteration:  42% 189/449 [01:14<01:44,  2.49it/s]\u001b[A\n",
            "Iteration:  42% 190/449 [01:14<01:44,  2.49it/s]\u001b[A\n",
            "Iteration:  43% 191/449 [01:15<01:43,  2.49it/s]\u001b[A\n",
            "Iteration:  43% 192/449 [01:15<01:43,  2.49it/s]\u001b[A\n",
            "Iteration:  43% 193/449 [01:16<01:42,  2.49it/s]\u001b[A\n",
            "Iteration:  43% 194/449 [01:16<01:42,  2.49it/s]\u001b[A\n",
            "Iteration:  43% 195/449 [01:16<01:41,  2.49it/s]\u001b[A\n",
            "Iteration:  44% 196/449 [01:17<01:41,  2.49it/s]\u001b[A\n",
            "Iteration:  44% 197/449 [01:17<01:41,  2.48it/s]\u001b[A\n",
            "Iteration:  44% 198/449 [01:18<01:40,  2.49it/s]\u001b[A\n",
            "Iteration:  44% 199/449 [01:18<01:40,  2.49it/s]\u001b[A\n",
            "Iteration:  45% 200/449 [01:18<01:40,  2.49it/s]\u001b[A\n",
            "Iteration:  45% 201/449 [01:19<01:39,  2.49it/s]\u001b[A\n",
            "Iteration:  45% 202/449 [01:19<01:39,  2.48it/s]\u001b[A\n",
            "Iteration:  45% 203/449 [01:20<01:39,  2.48it/s]\u001b[A\n",
            "Iteration:  45% 204/449 [01:20<01:38,  2.48it/s]\u001b[A\n",
            "Iteration:  46% 205/449 [01:20<01:38,  2.48it/s]\u001b[A\n",
            "Iteration:  46% 206/449 [01:21<01:37,  2.49it/s]\u001b[A\n",
            "Iteration:  46% 207/449 [01:21<01:37,  2.49it/s]\u001b[A\n",
            "Iteration:  46% 208/449 [01:22<01:36,  2.49it/s]\u001b[A\n",
            "Iteration:  47% 209/449 [01:22<01:36,  2.49it/s]\u001b[A\n",
            "Iteration:  47% 210/449 [01:22<01:36,  2.48it/s]\u001b[A\n",
            "Iteration:  47% 211/449 [01:23<01:35,  2.49it/s]\u001b[A\n",
            "Iteration:  47% 212/449 [01:23<01:35,  2.48it/s]\u001b[A\n",
            "Iteration:  47% 213/449 [01:24<01:35,  2.48it/s]\u001b[A\n",
            "Iteration:  48% 214/449 [01:24<01:34,  2.48it/s]\u001b[A\n",
            "Iteration:  48% 215/449 [01:24<01:34,  2.48it/s]\u001b[A\n",
            "Iteration:  48% 216/449 [01:25<01:33,  2.48it/s]\u001b[A\n",
            "Iteration:  48% 217/449 [01:25<01:33,  2.48it/s]\u001b[A\n",
            "Iteration:  49% 218/449 [01:26<01:33,  2.48it/s]\u001b[A\n",
            "Iteration:  49% 219/449 [01:26<01:32,  2.48it/s]\u001b[A\n",
            "Iteration:  49% 220/449 [01:26<01:32,  2.48it/s]\u001b[A\n",
            "Iteration:  49% 221/449 [01:27<01:32,  2.47it/s]\u001b[A\n",
            "Iteration:  49% 222/449 [01:27<01:31,  2.47it/s]\u001b[A\n",
            "Iteration:  50% 223/449 [01:28<01:31,  2.47it/s]\u001b[A\n",
            "Iteration:  50% 224/449 [01:28<01:31,  2.47it/s]\u001b[A\n",
            "Iteration:  50% 225/449 [01:28<01:30,  2.47it/s]\u001b[A\n",
            "Iteration:  50% 226/449 [01:29<01:30,  2.47it/s]\u001b[A\n",
            "Iteration:  51% 227/449 [01:29<01:30,  2.46it/s]\u001b[A\n",
            "Iteration:  51% 228/449 [01:30<01:29,  2.47it/s]\u001b[A\n",
            "Iteration:  51% 229/449 [01:30<01:29,  2.47it/s]\u001b[A\n",
            "Iteration:  51% 230/449 [01:30<01:28,  2.47it/s]\u001b[A\n",
            "Iteration:  51% 231/449 [01:31<01:28,  2.47it/s]\u001b[A\n",
            "Iteration:  52% 232/449 [01:31<01:27,  2.47it/s]\u001b[A\n",
            "Iteration:  52% 233/449 [01:32<01:27,  2.47it/s]\u001b[A\n",
            "Iteration:  52% 234/449 [01:32<01:27,  2.47it/s]\u001b[A\n",
            "Iteration:  52% 235/449 [01:32<01:26,  2.47it/s]\u001b[A\n",
            "Iteration:  53% 236/449 [01:33<01:26,  2.47it/s]\u001b[A\n",
            "Iteration:  53% 237/449 [01:33<01:25,  2.47it/s]\u001b[A\n",
            "Iteration:  53% 238/449 [01:34<01:25,  2.47it/s]\u001b[A\n",
            "Iteration:  53% 239/449 [01:34<01:25,  2.46it/s]\u001b[A\n",
            "Iteration:  53% 240/449 [01:34<01:24,  2.47it/s]\u001b[A\n",
            "Iteration:  54% 241/449 [01:35<01:24,  2.47it/s]\u001b[A\n",
            "Iteration:  54% 242/449 [01:35<01:24,  2.46it/s]\u001b[A\n",
            "Iteration:  54% 243/449 [01:36<01:23,  2.46it/s]\u001b[A\n",
            "Iteration:  54% 244/449 [01:36<01:23,  2.46it/s]\u001b[A\n",
            "Iteration:  55% 245/449 [01:37<01:23,  2.46it/s]\u001b[A\n",
            "Iteration:  55% 246/449 [01:37<01:22,  2.46it/s]\u001b[A\n",
            "Iteration:  55% 247/449 [01:37<01:21,  2.46it/s]\u001b[A\n",
            "Iteration:  55% 248/449 [01:38<01:21,  2.46it/s]\u001b[A\n",
            "Iteration:  55% 249/449 [01:38<01:21,  2.46it/s]\u001b[A\n",
            "Iteration:  56% 250/449 [01:39<01:21,  2.45it/s]\u001b[A\n",
            "Iteration:  56% 251/449 [01:39<01:20,  2.45it/s]\u001b[A\n",
            "Iteration:  56% 252/449 [01:39<01:20,  2.45it/s]\u001b[A\n",
            "Iteration:  56% 253/449 [01:40<01:19,  2.46it/s]\u001b[A\n",
            "Iteration:  57% 254/449 [01:40<01:19,  2.46it/s]\u001b[A\n",
            "Iteration:  57% 255/449 [01:41<01:18,  2.47it/s]\u001b[A\n",
            "Iteration:  57% 256/449 [01:41<01:18,  2.47it/s]\u001b[A\n",
            "Iteration:  57% 257/449 [01:41<01:17,  2.47it/s]\u001b[A\n",
            "Iteration:  57% 258/449 [01:42<01:17,  2.47it/s]\u001b[A\n",
            "Iteration:  58% 259/449 [01:42<01:16,  2.47it/s]\u001b[A\n",
            "Iteration:  58% 260/449 [01:43<01:16,  2.47it/s]\u001b[A\n",
            "Iteration:  58% 261/449 [01:43<01:16,  2.46it/s]\u001b[A\n",
            "Iteration:  58% 262/449 [01:43<01:15,  2.46it/s]\u001b[A\n",
            "Iteration:  59% 263/449 [01:44<01:15,  2.46it/s]\u001b[A\n",
            "Iteration:  59% 264/449 [01:44<01:15,  2.46it/s]\u001b[A\n",
            "Iteration:  59% 265/449 [01:45<01:14,  2.46it/s]\u001b[A\n",
            "Iteration:  59% 266/449 [01:45<01:14,  2.46it/s]\u001b[A\n",
            "Iteration:  59% 267/449 [01:45<01:13,  2.46it/s]\u001b[A\n",
            "Iteration:  60% 268/449 [01:46<01:13,  2.46it/s]\u001b[A\n",
            "Iteration:  60% 269/449 [01:46<01:13,  2.46it/s]\u001b[A\n",
            "Iteration:  60% 270/449 [01:47<01:12,  2.46it/s]\u001b[A\n",
            "Iteration:  60% 271/449 [01:47<01:12,  2.46it/s]\u001b[A\n",
            "Iteration:  61% 272/449 [01:47<01:11,  2.46it/s]\u001b[A\n",
            "Iteration:  61% 273/449 [01:48<01:11,  2.46it/s]\u001b[A\n",
            "Iteration:  61% 274/449 [01:48<01:11,  2.46it/s]\u001b[A\n",
            "Iteration:  61% 275/449 [01:49<01:10,  2.46it/s]\u001b[A\n",
            "Iteration:  61% 276/449 [01:49<01:10,  2.45it/s]\u001b[A\n",
            "Iteration:  62% 277/449 [01:50<01:10,  2.45it/s]\u001b[A\n",
            "Iteration:  62% 278/449 [01:50<01:09,  2.45it/s]\u001b[A\n",
            "Iteration:  62% 279/449 [01:50<01:09,  2.45it/s]\u001b[A\n",
            "Iteration:  62% 280/449 [01:51<01:08,  2.45it/s]\u001b[A\n",
            "Iteration:  63% 281/449 [01:51<01:08,  2.45it/s]\u001b[A\n",
            "Iteration:  63% 282/449 [01:52<01:08,  2.45it/s]\u001b[A\n",
            "Iteration:  63% 283/449 [01:52<01:07,  2.46it/s]\u001b[A\n",
            "Iteration:  63% 284/449 [01:52<01:07,  2.46it/s]\u001b[A\n",
            "Iteration:  63% 285/449 [01:53<01:06,  2.45it/s]\u001b[A\n",
            "Iteration:  64% 286/449 [01:53<01:06,  2.45it/s]\u001b[A\n",
            "Iteration:  64% 287/449 [01:54<01:06,  2.45it/s]\u001b[A\n",
            "Iteration:  64% 288/449 [01:54<01:05,  2.45it/s]\u001b[A\n",
            "Iteration:  64% 289/449 [01:54<01:05,  2.45it/s]\u001b[A\n",
            "Iteration:  65% 290/449 [01:55<01:04,  2.45it/s]\u001b[A\n",
            "Iteration:  65% 291/449 [01:55<01:04,  2.45it/s]\u001b[A\n",
            "Iteration:  65% 292/449 [01:56<01:04,  2.45it/s]\u001b[A\n",
            "Iteration:  65% 293/449 [01:56<01:03,  2.45it/s]\u001b[A\n",
            "Iteration:  65% 294/449 [01:56<01:03,  2.45it/s]\u001b[A\n",
            "Iteration:  66% 295/449 [01:57<01:02,  2.45it/s]\u001b[A\n",
            "Iteration:  66% 296/449 [01:57<01:02,  2.45it/s]\u001b[A\n",
            "Iteration:  66% 297/449 [01:58<01:02,  2.44it/s]\u001b[A\n",
            "Iteration:  66% 298/449 [01:58<01:01,  2.45it/s]\u001b[A\n",
            "Iteration:  67% 299/449 [01:59<01:01,  2.45it/s]\u001b[A\n",
            "Iteration:  67% 300/449 [01:59<01:00,  2.45it/s]\u001b[A\n",
            "Iteration:  67% 301/449 [01:59<01:00,  2.45it/s]\u001b[A\n",
            "Iteration:  67% 302/449 [02:00<01:00,  2.44it/s]\u001b[A\n",
            "Iteration:  67% 303/449 [02:00<00:59,  2.45it/s]\u001b[A\n",
            "Iteration:  68% 304/449 [02:01<00:59,  2.44it/s]\u001b[A\n",
            "Iteration:  68% 305/449 [02:01<00:58,  2.45it/s]\u001b[A\n",
            "Iteration:  68% 306/449 [02:01<00:58,  2.45it/s]\u001b[A\n",
            "Iteration:  68% 307/449 [02:02<00:58,  2.44it/s]\u001b[A\n",
            "Iteration:  69% 308/449 [02:02<00:57,  2.44it/s]\u001b[A\n",
            "Iteration:  69% 309/449 [02:03<00:57,  2.44it/s]\u001b[A\n",
            "Iteration:  69% 310/449 [02:03<00:57,  2.44it/s]\u001b[A\n",
            "Iteration:  69% 311/449 [02:03<00:56,  2.44it/s]\u001b[A\n",
            "Iteration:  69% 312/449 [02:04<00:56,  2.44it/s]\u001b[A\n",
            "Iteration:  70% 313/449 [02:04<00:55,  2.44it/s]\u001b[A\n",
            "Iteration:  70% 314/449 [02:05<00:55,  2.44it/s]\u001b[A\n",
            "Iteration:  70% 315/449 [02:05<00:55,  2.44it/s]\u001b[A\n",
            "Iteration:  70% 316/449 [02:05<00:54,  2.44it/s]\u001b[A\n",
            "Iteration:  71% 317/449 [02:06<00:54,  2.44it/s]\u001b[A\n",
            "Iteration:  71% 318/449 [02:06<00:53,  2.44it/s]\u001b[A\n",
            "Iteration:  71% 319/449 [02:07<00:53,  2.44it/s]\u001b[A\n",
            "Iteration:  71% 320/449 [02:07<00:52,  2.44it/s]\u001b[A\n",
            "Iteration:  71% 321/449 [02:08<00:52,  2.44it/s]\u001b[A\n",
            "Iteration:  72% 322/449 [02:08<00:52,  2.44it/s]\u001b[A\n",
            "Iteration:  72% 323/449 [02:08<00:51,  2.44it/s]\u001b[A\n",
            "Iteration:  72% 324/449 [02:09<00:51,  2.44it/s]\u001b[A\n",
            "Iteration:  72% 325/449 [02:09<00:50,  2.44it/s]\u001b[A\n",
            "Iteration:  73% 326/449 [02:10<00:50,  2.44it/s]\u001b[A\n",
            "Iteration:  73% 327/449 [02:10<00:50,  2.44it/s]\u001b[A\n",
            "Iteration:  73% 328/449 [02:10<00:49,  2.44it/s]\u001b[A\n",
            "Iteration:  73% 329/449 [02:11<00:49,  2.44it/s]\u001b[A\n",
            "Iteration:  73% 330/449 [02:11<00:48,  2.44it/s]\u001b[A\n",
            "Iteration:  74% 331/449 [02:12<00:48,  2.44it/s]\u001b[A\n",
            "Iteration:  74% 332/449 [02:12<00:47,  2.44it/s]\u001b[A\n",
            "Iteration:  74% 333/449 [02:12<00:47,  2.44it/s]\u001b[A\n",
            "Iteration:  74% 334/449 [02:13<00:47,  2.44it/s]\u001b[A\n",
            "Iteration:  75% 335/449 [02:13<00:46,  2.43it/s]\u001b[A\n",
            "Iteration:  75% 336/449 [02:14<00:46,  2.43it/s]\u001b[A\n",
            "Iteration:  75% 337/449 [02:14<00:46,  2.43it/s]\u001b[A\n",
            "Iteration:  75% 338/449 [02:14<00:45,  2.43it/s]\u001b[A\n",
            "Iteration:  76% 339/449 [02:15<00:45,  2.43it/s]\u001b[A\n",
            "Iteration:  76% 340/449 [02:15<00:44,  2.43it/s]\u001b[A\n",
            "Iteration:  76% 341/449 [02:16<00:44,  2.43it/s]\u001b[A\n",
            "Iteration:  76% 342/449 [02:16<00:43,  2.44it/s]\u001b[A\n",
            "Iteration:  76% 343/449 [02:17<00:43,  2.44it/s]\u001b[A\n",
            "Iteration:  77% 344/449 [02:17<00:43,  2.43it/s]\u001b[A\n",
            "Iteration:  77% 345/449 [02:17<00:42,  2.44it/s]\u001b[A\n",
            "Iteration:  77% 346/449 [02:18<00:42,  2.43it/s]\u001b[A\n",
            "Iteration:  77% 347/449 [02:18<00:41,  2.43it/s]\u001b[A\n",
            "Iteration:  78% 348/449 [02:19<00:41,  2.43it/s]\u001b[A\n",
            "Iteration:  78% 349/449 [02:19<00:41,  2.43it/s]\u001b[A\n",
            "Iteration:  78% 350/449 [02:19<00:40,  2.43it/s]\u001b[A\n",
            "Iteration:  78% 351/449 [02:20<00:40,  2.43it/s]\u001b[A\n",
            "Iteration:  78% 352/449 [02:20<00:39,  2.43it/s]\u001b[A\n",
            "Iteration:  79% 353/449 [02:21<00:39,  2.42it/s]\u001b[A\n",
            "Iteration:  79% 354/449 [02:21<00:39,  2.43it/s]\u001b[A\n",
            "Iteration:  79% 355/449 [02:21<00:38,  2.43it/s]\u001b[A\n",
            "Iteration:  79% 356/449 [02:22<00:38,  2.42it/s]\u001b[A\n",
            "Iteration:  80% 357/449 [02:22<00:37,  2.42it/s]\u001b[A\n",
            "Iteration:  80% 358/449 [02:23<00:37,  2.42it/s]\u001b[A\n",
            "Iteration:  80% 359/449 [02:23<00:37,  2.43it/s]\u001b[A\n",
            "Iteration:  80% 360/449 [02:24<00:36,  2.43it/s]\u001b[A\n",
            "Iteration:  80% 361/449 [02:24<00:36,  2.42it/s]\u001b[A\n",
            "Iteration:  81% 362/449 [02:24<00:35,  2.43it/s]\u001b[A\n",
            "Iteration:  81% 363/449 [02:25<00:35,  2.42it/s]\u001b[A\n",
            "Iteration:  81% 364/449 [02:25<00:35,  2.42it/s]\u001b[A\n",
            "Iteration:  81% 365/449 [02:26<00:34,  2.42it/s]\u001b[A\n",
            "Iteration:  82% 366/449 [02:26<00:34,  2.42it/s]\u001b[A\n",
            "Iteration:  82% 367/449 [02:26<00:33,  2.42it/s]\u001b[A\n",
            "Iteration:  82% 368/449 [02:27<00:33,  2.42it/s]\u001b[A\n",
            "Iteration:  82% 369/449 [02:27<00:33,  2.42it/s]\u001b[A\n",
            "Iteration:  82% 370/449 [02:28<00:32,  2.42it/s]\u001b[A\n",
            "Iteration:  83% 371/449 [02:28<00:32,  2.42it/s]\u001b[A\n",
            "Iteration:  83% 372/449 [02:29<00:31,  2.41it/s]\u001b[A\n",
            "Iteration:  83% 373/449 [02:29<00:31,  2.42it/s]\u001b[A\n",
            "Iteration:  83% 374/449 [02:29<00:31,  2.42it/s]\u001b[A\n",
            "Iteration:  84% 375/449 [02:30<00:30,  2.42it/s]\u001b[A\n",
            "Iteration:  84% 376/449 [02:30<00:30,  2.42it/s]\u001b[A\n",
            "Iteration:  84% 377/449 [02:31<00:29,  2.42it/s]\u001b[A\n",
            "Iteration:  84% 378/449 [02:31<00:29,  2.42it/s]\u001b[A\n",
            "Iteration:  84% 379/449 [02:31<00:28,  2.42it/s]\u001b[A\n",
            "Iteration:  85% 380/449 [02:32<00:28,  2.42it/s]\u001b[A\n",
            "Iteration:  85% 381/449 [02:32<00:28,  2.42it/s]\u001b[A\n",
            "Iteration:  85% 382/449 [02:33<00:27,  2.41it/s]\u001b[A\n",
            "Iteration:  85% 383/449 [02:33<00:27,  2.42it/s]\u001b[A\n",
            "Iteration:  86% 384/449 [02:33<00:26,  2.42it/s]\u001b[A\n",
            "Iteration:  86% 385/449 [02:34<00:26,  2.42it/s]\u001b[A\n",
            "Iteration:  86% 386/449 [02:34<00:26,  2.41it/s]\u001b[A\n",
            "Iteration:  86% 387/449 [02:35<00:25,  2.41it/s]\u001b[A\n",
            "Iteration:  86% 388/449 [02:35<00:25,  2.41it/s]\u001b[A\n",
            "Iteration:  87% 389/449 [02:36<00:24,  2.41it/s]\u001b[A\n",
            "Iteration:  87% 390/449 [02:36<00:24,  2.41it/s]\u001b[A\n",
            "Iteration:  87% 391/449 [02:36<00:24,  2.42it/s]\u001b[A\n",
            "Iteration:  87% 392/449 [02:37<00:23,  2.41it/s]\u001b[A\n",
            "Iteration:  88% 393/449 [02:37<00:23,  2.41it/s]\u001b[A\n",
            "Iteration:  88% 394/449 [02:38<00:22,  2.41it/s]\u001b[A\n",
            "Iteration:  88% 395/449 [02:38<00:22,  2.41it/s]\u001b[A\n",
            "Iteration:  88% 396/449 [02:38<00:21,  2.41it/s]\u001b[A\n",
            "Iteration:  88% 397/449 [02:39<00:21,  2.41it/s]\u001b[A\n",
            "Iteration:  89% 398/449 [02:39<00:21,  2.41it/s]\u001b[A\n",
            "Iteration:  89% 399/449 [02:40<00:20,  2.41it/s]\u001b[A\n",
            "Iteration:  89% 400/449 [02:40<00:20,  2.40it/s]\u001b[A\n",
            "Iteration:  89% 401/449 [02:41<00:19,  2.41it/s]\u001b[A\n",
            "Iteration:  90% 402/449 [02:41<00:19,  2.40it/s]\u001b[A\n",
            "Iteration:  90% 403/449 [02:41<00:19,  2.40it/s]\u001b[A\n",
            "Iteration:  90% 404/449 [02:42<00:18,  2.40it/s]\u001b[A\n",
            "Iteration:  90% 405/449 [02:42<00:18,  2.40it/s]\u001b[A\n",
            "Iteration:  90% 406/449 [02:43<00:17,  2.41it/s]\u001b[A\n",
            "Iteration:  91% 407/449 [02:43<00:17,  2.40it/s]\u001b[A\n",
            "Iteration:  91% 408/449 [02:43<00:17,  2.40it/s]\u001b[A\n",
            "Iteration:  91% 409/449 [02:44<00:16,  2.40it/s]\u001b[A\n",
            "Iteration:  91% 410/449 [02:44<00:16,  2.40it/s]\u001b[A\n",
            "Iteration:  92% 411/449 [02:45<00:15,  2.40it/s]\u001b[A\n",
            "Iteration:  92% 412/449 [02:45<00:15,  2.40it/s]\u001b[A\n",
            "Iteration:  92% 413/449 [02:46<00:14,  2.40it/s]\u001b[A\n",
            "Iteration:  92% 414/449 [02:46<00:14,  2.40it/s]\u001b[A\n",
            "Iteration:  92% 415/449 [02:46<00:14,  2.40it/s]\u001b[A\n",
            "Iteration:  93% 416/449 [02:47<00:13,  2.40it/s]\u001b[A\n",
            "Iteration:  93% 417/449 [02:47<00:13,  2.40it/s]\u001b[A\n",
            "Iteration:  93% 418/449 [02:48<00:12,  2.40it/s]\u001b[A\n",
            "Iteration:  93% 419/449 [02:48<00:12,  2.40it/s]\u001b[A\n",
            "Iteration:  94% 420/449 [02:48<00:12,  2.39it/s]\u001b[A\n",
            "Iteration:  94% 421/449 [02:49<00:11,  2.40it/s]\u001b[A\n",
            "Iteration:  94% 422/449 [02:49<00:11,  2.39it/s]\u001b[A\n",
            "Iteration:  94% 423/449 [02:50<00:10,  2.39it/s]\u001b[A\n",
            "Iteration:  94% 424/449 [02:50<00:10,  2.39it/s]\u001b[A\n",
            "Iteration:  95% 425/449 [02:51<00:10,  2.39it/s]\u001b[A\n",
            "Iteration:  95% 426/449 [02:51<00:09,  2.39it/s]\u001b[A\n",
            "Iteration:  95% 427/449 [02:51<00:09,  2.39it/s]\u001b[A\n",
            "Iteration:  95% 428/449 [02:52<00:08,  2.39it/s]\u001b[A\n",
            "Iteration:  96% 429/449 [02:52<00:08,  2.39it/s]\u001b[A\n",
            "Iteration:  96% 430/449 [02:53<00:07,  2.39it/s]\u001b[A\n",
            "Iteration:  96% 431/449 [02:53<00:07,  2.39it/s]\u001b[A\n",
            "Iteration:  96% 432/449 [02:53<00:07,  2.39it/s]\u001b[A\n",
            "Iteration:  96% 433/449 [02:54<00:06,  2.39it/s]\u001b[A\n",
            "Iteration:  97% 434/449 [02:54<00:06,  2.39it/s]\u001b[A\n",
            "Iteration:  97% 435/449 [02:55<00:05,  2.39it/s]\u001b[A\n",
            "Iteration:  97% 436/449 [02:55<00:05,  2.39it/s]\u001b[A\n",
            "Iteration:  97% 437/449 [02:56<00:05,  2.39it/s]\u001b[A\n",
            "Iteration:  98% 438/449 [02:56<00:04,  2.39it/s]\u001b[A\n",
            "Iteration:  98% 439/449 [02:56<00:04,  2.39it/s]\u001b[A\n",
            "Iteration:  98% 440/449 [02:57<00:03,  2.39it/s]\u001b[A\n",
            "Iteration:  98% 441/449 [02:57<00:03,  2.39it/s]\u001b[A\n",
            "Iteration:  98% 442/449 [02:58<00:02,  2.39it/s]\u001b[A\n",
            "Iteration:  99% 443/449 [02:58<00:02,  2.39it/s]\u001b[A\n",
            "Iteration:  99% 444/449 [02:59<00:02,  2.39it/s]\u001b[A\n",
            "Iteration:  99% 445/449 [02:59<00:01,  2.38it/s]\u001b[A\n",
            "Iteration:  99% 446/449 [02:59<00:01,  2.39it/s]\u001b[A\n",
            "Iteration: 100% 447/449 [03:00<00:00,  2.38it/s]\u001b[A\n",
            "Iteration: 100% 448/449 [03:00<00:00,  2.39it/s]\u001b[A\n",
            "Iteration: 100% 449/449 [03:01<00:00,  2.48it/s]\n",
            "Epoch: 100% 1/1 [03:01<00:00, 181.10s/it]\n",
            "03/21/2020 22:06:14 - INFO - __main__ -    global_step = 449, average loss = 3.364734341678747\n",
            "03/21/2020 22:06:14 - INFO - __main__ -   Saving model checkpoint to output_gpt_1940_1960_n75\n",
            "03/21/2020 22:06:14 - INFO - transformers.configuration_utils -   Configuration saved in output_gpt_1940_1960_n75/config.json\n",
            "03/21/2020 22:06:16 - INFO - transformers.modeling_utils -   Model weights saved in output_gpt_1940_1960_n75/pytorch_model.bin\n",
            "03/21/2020 22:06:16 - INFO - transformers.configuration_utils -   loading configuration file output_gpt_1940_1960_n75/config.json\n",
            "03/21/2020 22:06:16 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "03/21/2020 22:06:16 - INFO - transformers.modeling_utils -   loading weights file output_gpt_1940_1960_n75/pytorch_model.bin\n",
            "03/21/2020 22:06:19 - INFO - transformers.tokenization_utils -   Model name 'output_gpt_1940_1960_n75' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'output_gpt_1940_1960_n75' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/21/2020 22:06:19 - INFO - transformers.tokenization_utils -   Didn't find file output_gpt_1940_1960_n75/added_tokens.json. We won't load it.\n",
            "03/21/2020 22:06:19 - INFO - transformers.tokenization_utils -   loading file output_gpt_1940_1960_n75/vocab.json\n",
            "03/21/2020 22:06:19 - INFO - transformers.tokenization_utils -   loading file output_gpt_1940_1960_n75/merges.txt\n",
            "03/21/2020 22:06:19 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/21/2020 22:06:19 - INFO - transformers.tokenization_utils -   loading file output_gpt_1940_1960_n75/special_tokens_map.json\n",
            "03/21/2020 22:06:19 - INFO - transformers.tokenization_utils -   loading file output_gpt_1940_1960_n75/tokenizer_config.json\n",
            "03/21/2020 22:06:19 - INFO - __main__ -   Evaluate the following checkpoints: ['output_gpt_1940_1960_n75']\n",
            "03/21/2020 22:06:19 - INFO - transformers.configuration_utils -   loading configuration file output_gpt_1940_1960_n75/config.json\n",
            "03/21/2020 22:06:19 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "03/21/2020 22:06:19 - INFO - transformers.modeling_utils -   loading weights file output_gpt_1940_1960_n75/pytorch_model.bin\n",
            "03/21/2020 22:06:23 - INFO - __main__ -   Creating features from dataset file at /content\n",
            "03/21/2020 22:06:23 - INFO - __main__ -   Saving features into cached file /content/gpt2_cached_lm_1024_test_text_1940_1960_n75\n",
            "03/21/2020 22:06:23 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "03/21/2020 22:06:23 - INFO - __main__ -     Num examples = 111\n",
            "03/21/2020 22:06:23 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 111/111 [00:14<00:00,  7.79it/s]\n",
            "03/21/2020 22:06:38 - INFO - __main__ -   ***** Eval results  *****\n",
            "03/21/2020 22:06:38 - INFO - __main__ -     perplexity = tensor(26.1993)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2JV6qkwhNgO",
        "colab_type": "code",
        "outputId": "77a08973-3100-4a1a-dcc4-e98e23902783",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python run_generation.py --model_type=gpt2 --model_name_or_path=/content/output_gpt_1940_1960_n75"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/21/2020 22:06:43 - INFO - transformers.tokenization_utils -   Model name '/content/output_gpt_1940_1960_n75' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/output_gpt_1940_1960_n75' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/21/2020 22:06:43 - INFO - transformers.tokenization_utils -   Didn't find file /content/output_gpt_1940_1960_n75/added_tokens.json. We won't load it.\n",
            "03/21/2020 22:06:43 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_1940_1960_n75/vocab.json\n",
            "03/21/2020 22:06:43 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_1940_1960_n75/merges.txt\n",
            "03/21/2020 22:06:43 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/21/2020 22:06:43 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_1940_1960_n75/special_tokens_map.json\n",
            "03/21/2020 22:06:43 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_1940_1960_n75/tokenizer_config.json\n",
            "03/21/2020 22:06:43 - INFO - transformers.configuration_utils -   loading configuration file /content/output_gpt_1940_1960_n75/config.json\n",
            "03/21/2020 22:06:43 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "03/21/2020 22:06:43 - INFO - transformers.modeling_utils -   loading weights file /content/output_gpt_1940_1960_n75/pytorch_model.bin\n",
            "03/21/2020 22:06:50 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=0, length=20, model_name_or_path='/content/output_gpt_1940_1960_n75', model_type='gpt2', n_gpu=1, no_cuda=False, num_return_sequences=1, p=0.9, padding_text='', prompt='', repetition_penalty=1.0, seed=42, stop_token=None, temperature=1.0, xlm_language='')\n",
            "Model prompt >>> yes\n",
            "=== GENERATED SEQUENCE 1 ===\n",
            "yes what? Oh, it's not so bad. There's some really good coffee here. Let's\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zkBvmZthahV",
        "colab_type": "code",
        "outputId": "93a35c6b-6383-43b3-8c81-bc59ee1e633e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python /content/run_language_modelling.py --output_dir=output_gpt_2000_2020_n75 --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=/content/train_text_2000_2020_n75 --do_eval --eval_data_file=/content/test_text_2000_2020_n75 --per_gpu_train_batch_size=1 --per_gpu_eval_batch_size=1"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/22/2020 20:36:41 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "03/22/2020 20:36:41 - INFO - filelock -   Lock 140332811163688 acquired on /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942.lock\n",
            "03/22/2020 20:36:41 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmphwx3l6_i\n",
            "Downloading: 100% 224/224 [00:00<00:00, 268kB/s]\n",
            "03/22/2020 20:36:41 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json in cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n",
            "03/22/2020 20:36:41 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n",
            "03/22/2020 20:36:41 - INFO - filelock -   Lock 140332811163688 released on /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942.lock\n",
            "03/22/2020 20:36:41 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n",
            "03/22/2020 20:36:41 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "03/22/2020 20:36:42 - INFO - filelock -   Lock 140332811163688 acquired on /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock\n",
            "03/22/2020 20:36:42 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpbzc0culq\n",
            "Downloading: 100% 1.04M/1.04M [00:00<00:00, 2.94MB/s]\n",
            "03/22/2020 20:36:42 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json in cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "03/22/2020 20:36:42 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "03/22/2020 20:36:42 - INFO - filelock -   Lock 140332811163688 released on /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock\n",
            "03/22/2020 20:36:42 - INFO - filelock -   Lock 140332438363608 acquired on /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "03/22/2020 20:36:42 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp00mjk5wb\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 1.45MB/s]\n",
            "03/22/2020 20:36:43 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt in cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/22/2020 20:36:43 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/22/2020 20:36:43 - INFO - filelock -   Lock 140332438363608 released on /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "03/22/2020 20:36:43 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "03/22/2020 20:36:43 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/22/2020 20:36:43 - INFO - filelock -   Lock 140332438360696 acquired on /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1.lock\n",
            "03/22/2020 20:36:43 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpfaw6c36l\n",
            "Downloading: 100% 548M/548M [00:15<00:00, 36.3MB/s]\n",
            "03/22/2020 20:36:59 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin in cache at /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "03/22/2020 20:36:59 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "03/22/2020 20:36:59 - INFO - filelock -   Lock 140332438360696 released on /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1.lock\n",
            "03/22/2020 20:36:59 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "03/22/2020 20:37:12 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=1024, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='/content/test_text_2000_2020_n75', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='gpt2', model_type='gpt2', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='output_gpt_2000_2020_n75', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=1, per_gpu_train_batch_size=1, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='/content/train_text_2000_2020_n75', warmup_steps=0, weight_decay=0.0)\n",
            "03/22/2020 20:37:12 - INFO - __main__ -   Creating features from dataset file at /content\n",
            "03/22/2020 20:37:14 - INFO - __main__ -   Saving features into cached file /content/gpt2_cached_lm_1024_train_text_2000_2020_n75\n",
            "03/22/2020 20:37:14 - INFO - __main__ -   ***** Running training *****\n",
            "03/22/2020 20:37:14 - INFO - __main__ -     Num examples = 401\n",
            "03/22/2020 20:37:14 - INFO - __main__ -     Num Epochs = 1\n",
            "03/22/2020 20:37:14 - INFO - __main__ -     Instantaneous batch size per GPU = 1\n",
            "03/22/2020 20:37:14 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "03/22/2020 20:37:14 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "03/22/2020 20:37:14 - INFO - __main__ -     Total optimization steps = 401\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/401 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/401 [00:00<05:08,  1.30it/s]\u001b[A\n",
            "Iteration:   0% 2/401 [00:01<04:40,  1.42it/s]\u001b[A\n",
            "Iteration:   1% 3/401 [00:01<04:21,  1.52it/s]\u001b[A\n",
            "Iteration:   1% 4/401 [00:02<04:08,  1.60it/s]\u001b[A\n",
            "Iteration:   1% 5/401 [00:02<03:59,  1.65it/s]\u001b[A\n",
            "Iteration:   1% 6/401 [00:03<03:52,  1.70it/s]\u001b[A\n",
            "Iteration:   2% 7/401 [00:04<03:47,  1.73it/s]\u001b[A\n",
            "Iteration:   2% 8/401 [00:04<03:44,  1.75it/s]\u001b[A\n",
            "Iteration:   2% 9/401 [00:05<03:41,  1.77it/s]\u001b[A\n",
            "Iteration:   2% 10/401 [00:05<03:39,  1.78it/s]\u001b[A\n",
            "Iteration:   3% 11/401 [00:06<03:38,  1.79it/s]\u001b[A\n",
            "Iteration:   3% 12/401 [00:06<03:37,  1.79it/s]\u001b[A\n",
            "Iteration:   3% 13/401 [00:07<03:36,  1.80it/s]\u001b[A\n",
            "Iteration:   3% 14/401 [00:07<03:35,  1.79it/s]\u001b[A\n",
            "Iteration:   4% 15/401 [00:08<03:34,  1.80it/s]\u001b[A\n",
            "Iteration:   4% 16/401 [00:09<03:34,  1.80it/s]\u001b[A\n",
            "Iteration:   4% 17/401 [00:09<03:33,  1.80it/s]\u001b[A\n",
            "Iteration:   4% 18/401 [00:10<03:32,  1.80it/s]\u001b[A\n",
            "Iteration:   5% 19/401 [00:10<03:31,  1.80it/s]\u001b[A\n",
            "Iteration:   5% 20/401 [00:11<03:31,  1.80it/s]\u001b[A\n",
            "Iteration:   5% 21/401 [00:11<03:30,  1.80it/s]\u001b[A\n",
            "Iteration:   5% 22/401 [00:12<03:29,  1.80it/s]\u001b[A\n",
            "Iteration:   6% 23/401 [00:12<03:29,  1.81it/s]\u001b[A\n",
            "Iteration:   6% 24/401 [00:13<03:28,  1.81it/s]\u001b[A\n",
            "Iteration:   6% 25/401 [00:14<03:28,  1.80it/s]\u001b[A\n",
            "Iteration:   6% 26/401 [00:14<03:28,  1.80it/s]\u001b[A\n",
            "Iteration:   7% 27/401 [00:15<03:27,  1.80it/s]\u001b[A\n",
            "Iteration:   7% 28/401 [00:15<03:26,  1.80it/s]\u001b[A\n",
            "Iteration:   7% 29/401 [00:16<03:26,  1.80it/s]\u001b[A\n",
            "Iteration:   7% 30/401 [00:16<03:25,  1.80it/s]\u001b[A\n",
            "Iteration:   8% 31/401 [00:17<03:25,  1.80it/s]\u001b[A\n",
            "Iteration:   8% 32/401 [00:17<03:24,  1.80it/s]\u001b[A\n",
            "Iteration:   8% 33/401 [00:18<03:24,  1.80it/s]\u001b[A\n",
            "Iteration:   8% 34/401 [00:19<03:24,  1.80it/s]\u001b[A\n",
            "Iteration:   9% 35/401 [00:19<03:23,  1.80it/s]\u001b[A\n",
            "Iteration:   9% 36/401 [00:20<03:22,  1.80it/s]\u001b[A\n",
            "Iteration:   9% 37/401 [00:20<03:22,  1.80it/s]\u001b[A\n",
            "Iteration:   9% 38/401 [00:21<03:22,  1.80it/s]\u001b[A\n",
            "Iteration:  10% 39/401 [00:21<03:21,  1.80it/s]\u001b[A\n",
            "Iteration:  10% 40/401 [00:22<03:20,  1.80it/s]\u001b[A\n",
            "Iteration:  10% 41/401 [00:22<03:19,  1.80it/s]\u001b[A\n",
            "Iteration:  10% 42/401 [00:23<03:19,  1.80it/s]\u001b[A\n",
            "Iteration:  11% 43/401 [00:24<03:18,  1.80it/s]\u001b[A\n",
            "Iteration:  11% 44/401 [00:24<03:18,  1.80it/s]\u001b[A\n",
            "Iteration:  11% 45/401 [00:25<03:17,  1.80it/s]\u001b[A\n",
            "Iteration:  11% 46/401 [00:25<03:17,  1.80it/s]\u001b[A\n",
            "Iteration:  12% 47/401 [00:26<03:16,  1.80it/s]\u001b[A\n",
            "Iteration:  12% 48/401 [00:26<03:16,  1.80it/s]\u001b[A\n",
            "Iteration:  12% 49/401 [00:27<03:15,  1.80it/s]\u001b[A\n",
            "Iteration:  12% 50/401 [00:27<03:14,  1.80it/s]\u001b[A\n",
            "Iteration:  13% 51/401 [00:28<03:14,  1.80it/s]\u001b[A\n",
            "Iteration:  13% 52/401 [00:29<03:13,  1.80it/s]\u001b[A\n",
            "Iteration:  13% 53/401 [00:29<03:13,  1.80it/s]\u001b[A\n",
            "Iteration:  13% 54/401 [00:30<03:13,  1.80it/s]\u001b[A\n",
            "Iteration:  14% 55/401 [00:30<03:12,  1.80it/s]\u001b[A\n",
            "Iteration:  14% 56/401 [00:31<03:12,  1.79it/s]\u001b[A\n",
            "Iteration:  14% 57/401 [00:31<03:12,  1.79it/s]\u001b[A\n",
            "Iteration:  14% 58/401 [00:32<03:11,  1.79it/s]\u001b[A\n",
            "Iteration:  15% 59/401 [00:32<03:10,  1.79it/s]\u001b[A\n",
            "Iteration:  15% 60/401 [00:33<03:09,  1.80it/s]\u001b[A\n",
            "Iteration:  15% 61/401 [00:34<03:09,  1.80it/s]\u001b[A\n",
            "Iteration:  15% 62/401 [00:34<03:08,  1.80it/s]\u001b[A\n",
            "Iteration:  16% 63/401 [00:35<03:08,  1.80it/s]\u001b[A\n",
            "Iteration:  16% 64/401 [00:35<03:07,  1.80it/s]\u001b[A\n",
            "Iteration:  16% 65/401 [00:36<03:06,  1.80it/s]\u001b[A\n",
            "Iteration:  16% 66/401 [00:36<03:06,  1.80it/s]\u001b[A\n",
            "Iteration:  17% 67/401 [00:37<03:05,  1.80it/s]\u001b[A\n",
            "Iteration:  17% 68/401 [00:37<03:05,  1.79it/s]\u001b[A\n",
            "Iteration:  17% 69/401 [00:38<03:04,  1.80it/s]\u001b[A\n",
            "Iteration:  17% 70/401 [00:39<03:04,  1.80it/s]\u001b[A\n",
            "Iteration:  18% 71/401 [00:39<03:03,  1.79it/s]\u001b[A\n",
            "Iteration:  18% 72/401 [00:40<03:03,  1.80it/s]\u001b[A\n",
            "Iteration:  18% 73/401 [00:40<03:02,  1.79it/s]\u001b[A\n",
            "Iteration:  18% 74/401 [00:41<03:01,  1.80it/s]\u001b[A\n",
            "Iteration:  19% 75/401 [00:41<03:01,  1.80it/s]\u001b[A\n",
            "Iteration:  19% 76/401 [00:42<03:01,  1.79it/s]\u001b[A\n",
            "Iteration:  19% 77/401 [00:42<03:00,  1.79it/s]\u001b[A\n",
            "Iteration:  19% 78/401 [00:43<03:00,  1.79it/s]\u001b[A\n",
            "Iteration:  20% 79/401 [00:44<02:59,  1.80it/s]\u001b[A\n",
            "Iteration:  20% 80/401 [00:44<02:59,  1.79it/s]\u001b[A\n",
            "Iteration:  20% 81/401 [00:45<02:58,  1.79it/s]\u001b[A\n",
            "Iteration:  20% 82/401 [00:45<02:57,  1.79it/s]\u001b[A\n",
            "Iteration:  21% 83/401 [00:46<02:57,  1.79it/s]\u001b[A\n",
            "Iteration:  21% 84/401 [00:46<02:56,  1.79it/s]\u001b[A\n",
            "Iteration:  21% 85/401 [00:47<02:56,  1.79it/s]\u001b[A\n",
            "Iteration:  21% 86/401 [00:48<02:55,  1.79it/s]\u001b[A\n",
            "Iteration:  22% 87/401 [00:48<02:55,  1.79it/s]\u001b[A\n",
            "Iteration:  22% 88/401 [00:49<02:54,  1.79it/s]\u001b[A\n",
            "Iteration:  22% 89/401 [00:49<02:53,  1.80it/s]\u001b[A\n",
            "Iteration:  22% 90/401 [00:50<02:53,  1.80it/s]\u001b[A\n",
            "Iteration:  23% 91/401 [00:50<02:52,  1.80it/s]\u001b[A\n",
            "Iteration:  23% 92/401 [00:51<02:51,  1.80it/s]\u001b[A\n",
            "Iteration:  23% 93/401 [00:51<02:51,  1.80it/s]\u001b[A\n",
            "Iteration:  23% 94/401 [00:52<02:50,  1.80it/s]\u001b[A\n",
            "Iteration:  24% 95/401 [00:53<02:50,  1.80it/s]\u001b[A\n",
            "Iteration:  24% 96/401 [00:53<02:49,  1.80it/s]\u001b[A\n",
            "Iteration:  24% 97/401 [00:54<02:49,  1.80it/s]\u001b[A\n",
            "Iteration:  24% 98/401 [00:54<02:48,  1.79it/s]\u001b[A\n",
            "Iteration:  25% 99/401 [00:55<02:48,  1.79it/s]\u001b[A\n",
            "Iteration:  25% 100/401 [00:55<02:47,  1.80it/s]\u001b[A\n",
            "Iteration:  25% 101/401 [00:56<02:46,  1.80it/s]\u001b[A\n",
            "Iteration:  25% 102/401 [00:56<02:46,  1.80it/s]\u001b[A\n",
            "Iteration:  26% 103/401 [00:57<02:45,  1.80it/s]\u001b[A\n",
            "Iteration:  26% 104/401 [00:58<02:45,  1.79it/s]\u001b[A\n",
            "Iteration:  26% 105/401 [00:58<02:45,  1.79it/s]\u001b[A\n",
            "Iteration:  26% 106/401 [00:59<02:44,  1.79it/s]\u001b[A\n",
            "Iteration:  27% 107/401 [00:59<02:44,  1.79it/s]\u001b[A\n",
            "Iteration:  27% 108/401 [01:00<02:43,  1.79it/s]\u001b[A\n",
            "Iteration:  27% 109/401 [01:00<02:43,  1.79it/s]\u001b[A\n",
            "Iteration:  27% 110/401 [01:01<02:42,  1.79it/s]\u001b[A\n",
            "Iteration:  28% 111/401 [01:01<02:41,  1.79it/s]\u001b[A\n",
            "Iteration:  28% 112/401 [01:02<02:41,  1.79it/s]\u001b[A\n",
            "Iteration:  28% 113/401 [01:03<02:40,  1.79it/s]\u001b[A\n",
            "Iteration:  28% 114/401 [01:03<02:39,  1.79it/s]\u001b[A\n",
            "Iteration:  29% 115/401 [01:04<02:39,  1.79it/s]\u001b[A\n",
            "Iteration:  29% 116/401 [01:04<02:38,  1.79it/s]\u001b[A\n",
            "Iteration:  29% 117/401 [01:05<02:38,  1.79it/s]\u001b[A\n",
            "Iteration:  29% 118/401 [01:05<02:37,  1.79it/s]\u001b[A\n",
            "Iteration:  30% 119/401 [01:06<02:37,  1.79it/s]\u001b[A\n",
            "Iteration:  30% 120/401 [01:06<02:37,  1.79it/s]\u001b[A\n",
            "Iteration:  30% 121/401 [01:07<02:36,  1.79it/s]\u001b[A\n",
            "Iteration:  30% 122/401 [01:08<02:36,  1.78it/s]\u001b[A\n",
            "Iteration:  31% 123/401 [01:08<02:36,  1.78it/s]\u001b[A\n",
            "Iteration:  31% 124/401 [01:09<02:35,  1.78it/s]\u001b[A\n",
            "Iteration:  31% 125/401 [01:09<02:35,  1.78it/s]\u001b[A\n",
            "Iteration:  31% 126/401 [01:10<02:34,  1.78it/s]\u001b[A\n",
            "Iteration:  32% 127/401 [01:10<02:33,  1.78it/s]\u001b[A\n",
            "Iteration:  32% 128/401 [01:11<02:33,  1.78it/s]\u001b[A\n",
            "Iteration:  32% 129/401 [01:12<02:32,  1.78it/s]\u001b[A\n",
            "Iteration:  32% 130/401 [01:12<02:32,  1.78it/s]\u001b[A\n",
            "Iteration:  33% 131/401 [01:13<02:32,  1.77it/s]\u001b[A\n",
            "Iteration:  33% 132/401 [01:13<02:31,  1.77it/s]\u001b[A\n",
            "Iteration:  33% 133/401 [01:14<02:31,  1.77it/s]\u001b[A\n",
            "Iteration:  33% 134/401 [01:14<02:31,  1.77it/s]\u001b[A\n",
            "Iteration:  34% 135/401 [01:15<02:30,  1.77it/s]\u001b[A\n",
            "Iteration:  34% 136/401 [01:15<02:29,  1.77it/s]\u001b[A\n",
            "Iteration:  34% 137/401 [01:16<02:29,  1.77it/s]\u001b[A\n",
            "Iteration:  34% 138/401 [01:17<02:28,  1.78it/s]\u001b[A\n",
            "Iteration:  35% 139/401 [01:17<02:27,  1.78it/s]\u001b[A\n",
            "Iteration:  35% 140/401 [01:18<02:26,  1.78it/s]\u001b[A\n",
            "Iteration:  35% 141/401 [01:18<02:26,  1.78it/s]\u001b[A\n",
            "Iteration:  35% 142/401 [01:19<02:25,  1.78it/s]\u001b[A\n",
            "Iteration:  36% 143/401 [01:19<02:25,  1.78it/s]\u001b[A\n",
            "Iteration:  36% 144/401 [01:20<02:24,  1.78it/s]\u001b[A\n",
            "Iteration:  36% 145/401 [01:21<02:24,  1.78it/s]\u001b[A\n",
            "Iteration:  36% 146/401 [01:21<02:23,  1.78it/s]\u001b[A\n",
            "Iteration:  37% 147/401 [01:22<02:23,  1.78it/s]\u001b[A\n",
            "Iteration:  37% 148/401 [01:22<02:22,  1.78it/s]\u001b[A\n",
            "Iteration:  37% 149/401 [01:23<02:21,  1.78it/s]\u001b[A\n",
            "Iteration:  37% 150/401 [01:23<02:21,  1.78it/s]\u001b[A\n",
            "Iteration:  38% 151/401 [01:24<02:20,  1.78it/s]\u001b[A\n",
            "Iteration:  38% 152/401 [01:24<02:19,  1.78it/s]\u001b[A\n",
            "Iteration:  38% 153/401 [01:25<02:19,  1.78it/s]\u001b[A\n",
            "Iteration:  38% 154/401 [01:26<02:18,  1.78it/s]\u001b[A\n",
            "Iteration:  39% 155/401 [01:26<02:18,  1.78it/s]\u001b[A\n",
            "Iteration:  39% 156/401 [01:27<02:17,  1.78it/s]\u001b[A\n",
            "Iteration:  39% 157/401 [01:27<02:17,  1.77it/s]\u001b[A\n",
            "Iteration:  39% 158/401 [01:28<02:16,  1.77it/s]\u001b[A\n",
            "Iteration:  40% 159/401 [01:28<02:16,  1.78it/s]\u001b[A\n",
            "Iteration:  40% 160/401 [01:29<02:15,  1.78it/s]\u001b[A\n",
            "Iteration:  40% 161/401 [01:30<02:14,  1.78it/s]\u001b[A\n",
            "Iteration:  40% 162/401 [01:30<02:14,  1.78it/s]\u001b[A\n",
            "Iteration:  41% 163/401 [01:31<02:13,  1.78it/s]\u001b[A\n",
            "Iteration:  41% 164/401 [01:31<02:13,  1.78it/s]\u001b[A\n",
            "Iteration:  41% 165/401 [01:32<02:12,  1.78it/s]\u001b[A\n",
            "Iteration:  41% 166/401 [01:32<02:12,  1.78it/s]\u001b[A\n",
            "Iteration:  42% 167/401 [01:33<02:11,  1.78it/s]\u001b[A\n",
            "Iteration:  42% 168/401 [01:33<02:11,  1.78it/s]\u001b[A\n",
            "Iteration:  42% 169/401 [01:34<02:10,  1.77it/s]\u001b[A\n",
            "Iteration:  42% 170/401 [01:35<02:10,  1.77it/s]\u001b[A\n",
            "Iteration:  43% 171/401 [01:35<02:10,  1.77it/s]\u001b[A\n",
            "Iteration:  43% 172/401 [01:36<02:09,  1.76it/s]\u001b[A\n",
            "Iteration:  43% 173/401 [01:36<02:09,  1.76it/s]\u001b[A\n",
            "Iteration:  43% 174/401 [01:37<02:08,  1.77it/s]\u001b[A\n",
            "Iteration:  44% 175/401 [01:37<02:07,  1.77it/s]\u001b[A\n",
            "Iteration:  44% 176/401 [01:38<02:06,  1.77it/s]\u001b[A\n",
            "Iteration:  44% 177/401 [01:39<02:06,  1.77it/s]\u001b[A\n",
            "Iteration:  44% 178/401 [01:39<02:05,  1.77it/s]\u001b[A\n",
            "Iteration:  45% 179/401 [01:40<02:04,  1.78it/s]\u001b[A\n",
            "Iteration:  45% 180/401 [01:40<02:04,  1.78it/s]\u001b[A\n",
            "Iteration:  45% 181/401 [01:41<02:03,  1.78it/s]\u001b[A\n",
            "Iteration:  45% 182/401 [01:41<02:02,  1.78it/s]\u001b[A\n",
            "Iteration:  46% 183/401 [01:42<02:02,  1.78it/s]\u001b[A\n",
            "Iteration:  46% 184/401 [01:43<02:02,  1.77it/s]\u001b[A\n",
            "Iteration:  46% 185/401 [01:43<02:01,  1.77it/s]\u001b[A\n",
            "Iteration:  46% 186/401 [01:44<02:01,  1.77it/s]\u001b[A\n",
            "Iteration:  47% 187/401 [01:44<02:00,  1.77it/s]\u001b[A\n",
            "Iteration:  47% 188/401 [01:45<02:00,  1.77it/s]\u001b[A\n",
            "Iteration:  47% 189/401 [01:45<01:59,  1.78it/s]\u001b[A\n",
            "Iteration:  47% 190/401 [01:46<01:58,  1.78it/s]\u001b[A\n",
            "Iteration:  48% 191/401 [01:46<01:58,  1.77it/s]\u001b[A\n",
            "Iteration:  48% 192/401 [01:47<01:58,  1.77it/s]\u001b[A\n",
            "Iteration:  48% 193/401 [01:48<01:57,  1.77it/s]\u001b[A\n",
            "Iteration:  48% 194/401 [01:48<01:56,  1.78it/s]\u001b[A\n",
            "Iteration:  49% 195/401 [01:49<01:55,  1.78it/s]\u001b[A\n",
            "Iteration:  49% 196/401 [01:49<01:55,  1.77it/s]\u001b[A\n",
            "Iteration:  49% 197/401 [01:50<01:54,  1.78it/s]\u001b[A\n",
            "Iteration:  49% 198/401 [01:50<01:54,  1.78it/s]\u001b[A\n",
            "Iteration:  50% 199/401 [01:51<01:54,  1.77it/s]\u001b[A\n",
            "Iteration:  50% 200/401 [01:52<01:53,  1.77it/s]\u001b[A\n",
            "Iteration:  50% 201/401 [01:52<01:52,  1.77it/s]\u001b[A\n",
            "Iteration:  50% 202/401 [01:53<01:52,  1.77it/s]\u001b[A\n",
            "Iteration:  51% 203/401 [01:53<01:51,  1.77it/s]\u001b[A\n",
            "Iteration:  51% 204/401 [01:54<01:51,  1.77it/s]\u001b[A\n",
            "Iteration:  51% 205/401 [01:54<01:50,  1.77it/s]\u001b[A\n",
            "Iteration:  51% 206/401 [01:55<01:49,  1.78it/s]\u001b[A\n",
            "Iteration:  52% 207/401 [01:55<01:49,  1.77it/s]\u001b[A\n",
            "Iteration:  52% 208/401 [01:56<01:48,  1.78it/s]\u001b[A\n",
            "Iteration:  52% 209/401 [01:57<01:47,  1.78it/s]\u001b[A\n",
            "Iteration:  52% 210/401 [01:57<01:47,  1.78it/s]\u001b[A\n",
            "Iteration:  53% 211/401 [01:58<01:46,  1.78it/s]\u001b[A\n",
            "Iteration:  53% 212/401 [01:58<01:46,  1.78it/s]\u001b[A\n",
            "Iteration:  53% 213/401 [01:59<01:45,  1.78it/s]\u001b[A\n",
            "Iteration:  53% 214/401 [01:59<01:45,  1.78it/s]\u001b[A\n",
            "Iteration:  54% 215/401 [02:00<01:44,  1.78it/s]\u001b[A\n",
            "Iteration:  54% 216/401 [02:01<01:44,  1.78it/s]\u001b[A\n",
            "Iteration:  54% 217/401 [02:01<01:43,  1.78it/s]\u001b[A\n",
            "Iteration:  54% 218/401 [02:02<01:42,  1.78it/s]\u001b[A\n",
            "Iteration:  55% 219/401 [02:02<01:42,  1.78it/s]\u001b[A\n",
            "Iteration:  55% 220/401 [02:03<01:41,  1.78it/s]\u001b[A\n",
            "Iteration:  55% 221/401 [02:03<01:41,  1.78it/s]\u001b[A\n",
            "Iteration:  55% 222/401 [02:04<01:40,  1.78it/s]\u001b[A\n",
            "Iteration:  56% 223/401 [02:04<01:40,  1.78it/s]\u001b[A\n",
            "Iteration:  56% 224/401 [02:05<01:39,  1.77it/s]\u001b[A\n",
            "Iteration:  56% 225/401 [02:06<01:39,  1.77it/s]\u001b[A\n",
            "Iteration:  56% 226/401 [02:06<01:38,  1.77it/s]\u001b[A\n",
            "Iteration:  57% 227/401 [02:07<01:38,  1.77it/s]\u001b[A\n",
            "Iteration:  57% 228/401 [02:07<01:37,  1.77it/s]\u001b[A\n",
            "Iteration:  57% 229/401 [02:08<01:36,  1.77it/s]\u001b[A\n",
            "Iteration:  57% 230/401 [02:08<01:36,  1.77it/s]\u001b[A\n",
            "Iteration:  58% 231/401 [02:09<01:36,  1.77it/s]\u001b[A\n",
            "Iteration:  58% 232/401 [02:10<01:35,  1.77it/s]\u001b[A\n",
            "Iteration:  58% 233/401 [02:10<01:34,  1.77it/s]\u001b[A\n",
            "Iteration:  58% 234/401 [02:11<01:34,  1.77it/s]\u001b[A\n",
            "Iteration:  59% 235/401 [02:11<01:34,  1.76it/s]\u001b[A\n",
            "Iteration:  59% 236/401 [02:12<01:33,  1.76it/s]\u001b[A\n",
            "Iteration:  59% 237/401 [02:12<01:32,  1.77it/s]\u001b[A\n",
            "Iteration:  59% 238/401 [02:13<01:32,  1.76it/s]\u001b[A\n",
            "Iteration:  60% 239/401 [02:14<01:31,  1.76it/s]\u001b[A\n",
            "Iteration:  60% 240/401 [02:14<01:31,  1.76it/s]\u001b[A\n",
            "Iteration:  60% 241/401 [02:15<01:30,  1.76it/s]\u001b[A\n",
            "Iteration:  60% 242/401 [02:15<01:30,  1.76it/s]\u001b[A\n",
            "Iteration:  61% 243/401 [02:16<01:29,  1.76it/s]\u001b[A\n",
            "Iteration:  61% 244/401 [02:16<01:28,  1.77it/s]\u001b[A\n",
            "Iteration:  61% 245/401 [02:17<01:28,  1.77it/s]\u001b[A\n",
            "Iteration:  61% 246/401 [02:18<01:27,  1.77it/s]\u001b[A\n",
            "Iteration:  62% 247/401 [02:18<01:26,  1.77it/s]\u001b[A\n",
            "Iteration:  62% 248/401 [02:19<01:26,  1.77it/s]\u001b[A\n",
            "Iteration:  62% 249/401 [02:19<01:26,  1.76it/s]\u001b[A\n",
            "Iteration:  62% 250/401 [02:20<01:25,  1.76it/s]\u001b[A\n",
            "Iteration:  63% 251/401 [02:20<01:25,  1.76it/s]\u001b[A\n",
            "Iteration:  63% 252/401 [02:21<01:24,  1.76it/s]\u001b[A\n",
            "Iteration:  63% 253/401 [02:21<01:24,  1.76it/s]\u001b[A\n",
            "Iteration:  63% 254/401 [02:22<01:23,  1.77it/s]\u001b[A\n",
            "Iteration:  64% 255/401 [02:23<01:22,  1.77it/s]\u001b[A\n",
            "Iteration:  64% 256/401 [02:23<01:21,  1.77it/s]\u001b[A\n",
            "Iteration:  64% 257/401 [02:24<01:21,  1.77it/s]\u001b[A\n",
            "Iteration:  64% 258/401 [02:24<01:20,  1.77it/s]\u001b[A\n",
            "Iteration:  65% 259/401 [02:25<01:20,  1.76it/s]\u001b[A\n",
            "Iteration:  65% 260/401 [02:25<01:19,  1.76it/s]\u001b[A\n",
            "Iteration:  65% 261/401 [02:26<01:19,  1.76it/s]\u001b[A\n",
            "Iteration:  65% 262/401 [02:27<01:18,  1.77it/s]\u001b[A\n",
            "Iteration:  66% 263/401 [02:27<01:18,  1.76it/s]\u001b[A\n",
            "Iteration:  66% 264/401 [02:28<01:17,  1.76it/s]\u001b[A\n",
            "Iteration:  66% 265/401 [02:28<01:17,  1.76it/s]\u001b[A\n",
            "Iteration:  66% 266/401 [02:29<01:16,  1.77it/s]\u001b[A\n",
            "Iteration:  67% 267/401 [02:29<01:16,  1.76it/s]\u001b[A\n",
            "Iteration:  67% 268/401 [02:30<01:15,  1.76it/s]\u001b[A\n",
            "Iteration:  67% 269/401 [02:31<01:15,  1.76it/s]\u001b[A\n",
            "Iteration:  67% 270/401 [02:31<01:14,  1.75it/s]\u001b[A\n",
            "Iteration:  68% 271/401 [02:32<01:13,  1.76it/s]\u001b[A\n",
            "Iteration:  68% 272/401 [02:32<01:13,  1.76it/s]\u001b[A\n",
            "Iteration:  68% 273/401 [02:33<01:12,  1.76it/s]\u001b[A\n",
            "Iteration:  68% 274/401 [02:33<01:12,  1.76it/s]\u001b[A\n",
            "Iteration:  69% 275/401 [02:34<01:11,  1.77it/s]\u001b[A\n",
            "Iteration:  69% 276/401 [02:35<01:10,  1.77it/s]\u001b[A\n",
            "Iteration:  69% 277/401 [02:35<01:10,  1.77it/s]\u001b[A\n",
            "Iteration:  69% 278/401 [02:36<01:09,  1.77it/s]\u001b[A\n",
            "Iteration:  70% 279/401 [02:36<01:08,  1.77it/s]\u001b[A\n",
            "Iteration:  70% 280/401 [02:37<01:08,  1.75it/s]\u001b[A\n",
            "Iteration:  70% 281/401 [02:37<01:08,  1.76it/s]\u001b[A\n",
            "Iteration:  70% 282/401 [02:38<01:07,  1.75it/s]\u001b[A\n",
            "Iteration:  71% 283/401 [02:39<01:07,  1.76it/s]\u001b[A\n",
            "Iteration:  71% 284/401 [02:39<01:06,  1.76it/s]\u001b[A\n",
            "Iteration:  71% 285/401 [02:40<01:06,  1.75it/s]\u001b[A\n",
            "Iteration:  71% 286/401 [02:40<01:05,  1.76it/s]\u001b[A\n",
            "Iteration:  72% 287/401 [02:41<01:05,  1.75it/s]\u001b[A\n",
            "Iteration:  72% 288/401 [02:41<01:04,  1.75it/s]\u001b[A\n",
            "Iteration:  72% 289/401 [02:42<01:03,  1.75it/s]\u001b[A\n",
            "Iteration:  72% 290/401 [02:42<01:03,  1.76it/s]\u001b[A\n",
            "Iteration:  73% 291/401 [02:43<01:02,  1.76it/s]\u001b[A\n",
            "Iteration:  73% 292/401 [02:44<01:02,  1.75it/s]\u001b[A\n",
            "Iteration:  73% 293/401 [02:44<01:01,  1.76it/s]\u001b[A\n",
            "Iteration:  73% 294/401 [02:45<01:00,  1.76it/s]\u001b[A\n",
            "Iteration:  74% 295/401 [02:45<01:00,  1.76it/s]\u001b[A\n",
            "Iteration:  74% 296/401 [02:46<00:59,  1.76it/s]\u001b[A\n",
            "Iteration:  74% 297/401 [02:46<00:59,  1.76it/s]\u001b[A\n",
            "Iteration:  74% 298/401 [02:47<00:58,  1.75it/s]\u001b[A\n",
            "Iteration:  75% 299/401 [02:48<00:58,  1.76it/s]\u001b[A\n",
            "Iteration:  75% 300/401 [02:48<00:57,  1.76it/s]\u001b[A\n",
            "Iteration:  75% 301/401 [02:49<00:56,  1.76it/s]\u001b[A\n",
            "Iteration:  75% 302/401 [02:49<00:56,  1.76it/s]\u001b[A\n",
            "Iteration:  76% 303/401 [02:50<00:55,  1.76it/s]\u001b[A\n",
            "Iteration:  76% 304/401 [02:50<00:55,  1.76it/s]\u001b[A\n",
            "Iteration:  76% 305/401 [02:51<00:54,  1.76it/s]\u001b[A\n",
            "Iteration:  76% 306/401 [02:52<00:53,  1.76it/s]\u001b[A\n",
            "Iteration:  77% 307/401 [02:52<00:53,  1.76it/s]\u001b[A\n",
            "Iteration:  77% 308/401 [02:53<00:52,  1.76it/s]\u001b[A\n",
            "Iteration:  77% 309/401 [02:53<00:52,  1.75it/s]\u001b[A\n",
            "Iteration:  77% 310/401 [02:54<00:52,  1.75it/s]\u001b[A\n",
            "Iteration:  78% 311/401 [02:54<00:51,  1.76it/s]\u001b[A\n",
            "Iteration:  78% 312/401 [02:55<00:50,  1.76it/s]\u001b[A\n",
            "Iteration:  78% 313/401 [02:56<00:49,  1.76it/s]\u001b[A\n",
            "Iteration:  78% 314/401 [02:56<00:49,  1.76it/s]\u001b[A\n",
            "Iteration:  79% 315/401 [02:57<00:48,  1.76it/s]\u001b[A\n",
            "Iteration:  79% 316/401 [02:57<00:48,  1.76it/s]\u001b[A\n",
            "Iteration:  79% 317/401 [02:58<00:47,  1.76it/s]\u001b[A\n",
            "Iteration:  79% 318/401 [02:58<00:47,  1.76it/s]\u001b[A\n",
            "Iteration:  80% 319/401 [02:59<00:46,  1.76it/s]\u001b[A\n",
            "Iteration:  80% 320/401 [03:00<00:46,  1.75it/s]\u001b[A\n",
            "Iteration:  80% 321/401 [03:00<00:45,  1.76it/s]\u001b[A\n",
            "Iteration:  80% 322/401 [03:01<00:44,  1.76it/s]\u001b[A\n",
            "Iteration:  81% 323/401 [03:01<00:44,  1.76it/s]\u001b[A\n",
            "Iteration:  81% 324/401 [03:02<00:43,  1.76it/s]\u001b[A\n",
            "Iteration:  81% 325/401 [03:02<00:43,  1.76it/s]\u001b[A\n",
            "Iteration:  81% 326/401 [03:03<00:42,  1.76it/s]\u001b[A\n",
            "Iteration:  82% 327/401 [03:04<00:41,  1.76it/s]\u001b[A\n",
            "Iteration:  82% 328/401 [03:04<00:41,  1.75it/s]\u001b[A\n",
            "Iteration:  82% 329/401 [03:05<00:41,  1.75it/s]\u001b[A\n",
            "Iteration:  82% 330/401 [03:05<00:40,  1.75it/s]\u001b[A\n",
            "Iteration:  83% 331/401 [03:06<00:39,  1.76it/s]\u001b[A\n",
            "Iteration:  83% 332/401 [03:06<00:39,  1.76it/s]\u001b[A\n",
            "Iteration:  83% 333/401 [03:07<00:38,  1.75it/s]\u001b[A\n",
            "Iteration:  83% 334/401 [03:08<00:38,  1.76it/s]\u001b[A\n",
            "Iteration:  84% 335/401 [03:08<00:37,  1.76it/s]\u001b[A\n",
            "Iteration:  84% 336/401 [03:09<00:36,  1.76it/s]\u001b[A\n",
            "Iteration:  84% 337/401 [03:09<00:36,  1.76it/s]\u001b[A\n",
            "Iteration:  84% 338/401 [03:10<00:35,  1.76it/s]\u001b[A\n",
            "Iteration:  85% 339/401 [03:10<00:35,  1.76it/s]\u001b[A\n",
            "Iteration:  85% 340/401 [03:11<00:34,  1.77it/s]\u001b[A\n",
            "Iteration:  85% 341/401 [03:12<00:34,  1.75it/s]\u001b[A\n",
            "Iteration:  85% 342/401 [03:12<00:33,  1.76it/s]\u001b[A\n",
            "Iteration:  86% 343/401 [03:13<00:32,  1.76it/s]\u001b[A\n",
            "Iteration:  86% 344/401 [03:13<00:32,  1.76it/s]\u001b[A\n",
            "Iteration:  86% 345/401 [03:14<00:31,  1.76it/s]\u001b[A\n",
            "Iteration:  86% 346/401 [03:14<00:31,  1.76it/s]\u001b[A\n",
            "Iteration:  87% 347/401 [03:15<00:30,  1.77it/s]\u001b[A\n",
            "Iteration:  87% 348/401 [03:15<00:29,  1.77it/s]\u001b[A\n",
            "Iteration:  87% 349/401 [03:16<00:29,  1.77it/s]\u001b[A\n",
            "Iteration:  87% 350/401 [03:17<00:28,  1.77it/s]\u001b[A\n",
            "Iteration:  88% 351/401 [03:17<00:28,  1.77it/s]\u001b[A\n",
            "Iteration:  88% 352/401 [03:18<00:27,  1.77it/s]\u001b[A\n",
            "Iteration:  88% 353/401 [03:18<00:27,  1.76it/s]\u001b[A\n",
            "Iteration:  88% 354/401 [03:19<00:26,  1.76it/s]\u001b[A\n",
            "Iteration:  89% 355/401 [03:19<00:25,  1.77it/s]\u001b[A\n",
            "Iteration:  89% 356/401 [03:20<00:25,  1.77it/s]\u001b[A\n",
            "Iteration:  89% 357/401 [03:21<00:24,  1.77it/s]\u001b[A\n",
            "Iteration:  89% 358/401 [03:21<00:24,  1.77it/s]\u001b[A\n",
            "Iteration:  90% 359/401 [03:22<00:23,  1.77it/s]\u001b[A\n",
            "Iteration:  90% 360/401 [03:22<00:23,  1.77it/s]\u001b[A\n",
            "Iteration:  90% 361/401 [03:23<00:22,  1.77it/s]\u001b[A\n",
            "Iteration:  90% 362/401 [03:23<00:22,  1.76it/s]\u001b[A\n",
            "Iteration:  91% 363/401 [03:24<00:21,  1.76it/s]\u001b[A\n",
            "Iteration:  91% 364/401 [03:25<00:21,  1.76it/s]\u001b[A\n",
            "Iteration:  91% 365/401 [03:25<00:20,  1.76it/s]\u001b[A\n",
            "Iteration:  91% 366/401 [03:26<00:19,  1.77it/s]\u001b[A\n",
            "Iteration:  92% 367/401 [03:26<00:19,  1.77it/s]\u001b[A\n",
            "Iteration:  92% 368/401 [03:27<00:18,  1.77it/s]\u001b[A\n",
            "Iteration:  92% 369/401 [03:27<00:18,  1.77it/s]\u001b[A\n",
            "Iteration:  92% 370/401 [03:28<00:17,  1.76it/s]\u001b[A\n",
            "Iteration:  93% 371/401 [03:28<00:16,  1.77it/s]\u001b[A\n",
            "Iteration:  93% 372/401 [03:29<00:16,  1.77it/s]\u001b[A\n",
            "Iteration:  93% 373/401 [03:30<00:15,  1.77it/s]\u001b[A\n",
            "Iteration:  93% 374/401 [03:30<00:15,  1.76it/s]\u001b[A\n",
            "Iteration:  94% 375/401 [03:31<00:14,  1.76it/s]\u001b[A\n",
            "Iteration:  94% 376/401 [03:31<00:14,  1.77it/s]\u001b[A\n",
            "Iteration:  94% 377/401 [03:32<00:13,  1.76it/s]\u001b[A\n",
            "Iteration:  94% 378/401 [03:32<00:13,  1.75it/s]\u001b[A\n",
            "Iteration:  95% 379/401 [03:33<00:12,  1.75it/s]\u001b[A\n",
            "Iteration:  95% 380/401 [03:34<00:11,  1.75it/s]\u001b[A\n",
            "Iteration:  95% 381/401 [03:34<00:11,  1.76it/s]\u001b[A\n",
            "Iteration:  95% 382/401 [03:35<00:10,  1.76it/s]\u001b[A\n",
            "Iteration:  96% 383/401 [03:35<00:10,  1.75it/s]\u001b[A\n",
            "Iteration:  96% 384/401 [03:36<00:09,  1.75it/s]\u001b[A\n",
            "Iteration:  96% 385/401 [03:36<00:09,  1.75it/s]\u001b[A\n",
            "Iteration:  96% 386/401 [03:37<00:08,  1.76it/s]\u001b[A\n",
            "Iteration:  97% 387/401 [03:38<00:07,  1.76it/s]\u001b[A\n",
            "Iteration:  97% 388/401 [03:38<00:07,  1.75it/s]\u001b[A\n",
            "Iteration:  97% 389/401 [03:39<00:06,  1.75it/s]\u001b[A\n",
            "Iteration:  97% 390/401 [03:39<00:06,  1.76it/s]\u001b[A\n",
            "Iteration:  98% 391/401 [03:40<00:05,  1.76it/s]\u001b[A\n",
            "Iteration:  98% 392/401 [03:40<00:05,  1.76it/s]\u001b[A\n",
            "Iteration:  98% 393/401 [03:41<00:04,  1.75it/s]\u001b[A\n",
            "Iteration:  98% 394/401 [03:42<00:03,  1.76it/s]\u001b[A\n",
            "Iteration:  99% 395/401 [03:42<00:03,  1.76it/s]\u001b[A\n",
            "Iteration:  99% 396/401 [03:43<00:02,  1.76it/s]\u001b[A\n",
            "Iteration:  99% 397/401 [03:43<00:02,  1.76it/s]\u001b[A\n",
            "Iteration:  99% 398/401 [03:44<00:01,  1.74it/s]\u001b[A\n",
            "Iteration: 100% 399/401 [03:44<00:01,  1.74it/s]\u001b[A\n",
            "Iteration: 100% 400/401 [03:45<00:00,  1.74it/s]\u001b[A\n",
            "Iteration: 100% 401/401 [03:46<00:00,  1.77it/s]\n",
            "Epoch: 100% 1/1 [03:46<00:00, 226.07s/it]\n",
            "03/22/2020 20:41:00 - INFO - __main__ -    global_step = 401, average loss = 3.3427702369832635\n",
            "03/22/2020 20:41:00 - INFO - __main__ -   Saving model checkpoint to output_gpt_2000_2020_n75\n",
            "03/22/2020 20:41:00 - INFO - transformers.configuration_utils -   Configuration saved in output_gpt_2000_2020_n75/config.json\n",
            "03/22/2020 20:41:01 - INFO - transformers.modeling_utils -   Model weights saved in output_gpt_2000_2020_n75/pytorch_model.bin\n",
            "03/22/2020 20:41:01 - INFO - transformers.configuration_utils -   loading configuration file output_gpt_2000_2020_n75/config.json\n",
            "03/22/2020 20:41:01 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "03/22/2020 20:41:01 - INFO - transformers.modeling_utils -   loading weights file output_gpt_2000_2020_n75/pytorch_model.bin\n",
            "03/22/2020 20:41:05 - INFO - transformers.tokenization_utils -   Model name 'output_gpt_2000_2020_n75' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'output_gpt_2000_2020_n75' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/22/2020 20:41:05 - INFO - transformers.tokenization_utils -   Didn't find file output_gpt_2000_2020_n75/added_tokens.json. We won't load it.\n",
            "03/22/2020 20:41:05 - INFO - transformers.tokenization_utils -   loading file output_gpt_2000_2020_n75/vocab.json\n",
            "03/22/2020 20:41:05 - INFO - transformers.tokenization_utils -   loading file output_gpt_2000_2020_n75/merges.txt\n",
            "03/22/2020 20:41:05 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/22/2020 20:41:05 - INFO - transformers.tokenization_utils -   loading file output_gpt_2000_2020_n75/special_tokens_map.json\n",
            "03/22/2020 20:41:05 - INFO - transformers.tokenization_utils -   loading file output_gpt_2000_2020_n75/tokenizer_config.json\n",
            "03/22/2020 20:41:05 - INFO - __main__ -   Evaluate the following checkpoints: ['output_gpt_2000_2020_n75']\n",
            "03/22/2020 20:41:05 - INFO - transformers.configuration_utils -   loading configuration file output_gpt_2000_2020_n75/config.json\n",
            "03/22/2020 20:41:05 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "03/22/2020 20:41:05 - INFO - transformers.modeling_utils -   loading weights file output_gpt_2000_2020_n75/pytorch_model.bin\n",
            "03/22/2020 20:41:09 - INFO - __main__ -   Creating features from dataset file at /content\n",
            "03/22/2020 20:41:09 - INFO - __main__ -   Saving features into cached file /content/gpt2_cached_lm_1024_test_text_2000_2020_n75\n",
            "03/22/2020 20:41:09 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "03/22/2020 20:41:09 - INFO - __main__ -     Num examples = 129\n",
            "03/22/2020 20:41:09 - INFO - __main__ -     Batch size = 1\n",
            "Evaluating: 100% 129/129 [00:21<00:00,  5.97it/s]\n",
            "03/22/2020 20:41:31 - INFO - __main__ -   ***** Eval results  *****\n",
            "03/22/2020 20:41:31 - INFO - __main__ -     perplexity = tensor(23.9230)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4BbARzThbSD",
        "colab_type": "code",
        "outputId": "c3e4a1bb-c8ea-44f4-9fa1-330ab89d0b44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python run_generation.py --model_type=gpt2 --model_name_or_path=/content/output_gpt_2000_2020_n75"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/22/2020 20:41:35 - INFO - transformers.tokenization_utils -   Model name '/content/output_gpt_2000_2020_n75' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/output_gpt_2000_2020_n75' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "03/22/2020 20:41:35 - INFO - transformers.tokenization_utils -   Didn't find file /content/output_gpt_2000_2020_n75/added_tokens.json. We won't load it.\n",
            "03/22/2020 20:41:35 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_2000_2020_n75/vocab.json\n",
            "03/22/2020 20:41:35 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_2000_2020_n75/merges.txt\n",
            "03/22/2020 20:41:35 - INFO - transformers.tokenization_utils -   loading file None\n",
            "03/22/2020 20:41:35 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_2000_2020_n75/special_tokens_map.json\n",
            "03/22/2020 20:41:35 - INFO - transformers.tokenization_utils -   loading file /content/output_gpt_2000_2020_n75/tokenizer_config.json\n",
            "03/22/2020 20:41:36 - INFO - transformers.configuration_utils -   loading configuration file /content/output_gpt_2000_2020_n75/config.json\n",
            "03/22/2020 20:41:36 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": null,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "03/22/2020 20:41:36 - INFO - transformers.modeling_utils -   loading weights file /content/output_gpt_2000_2020_n75/pytorch_model.bin\n",
            "03/22/2020 20:41:42 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=0, length=20, model_name_or_path='/content/output_gpt_2000_2020_n75', model_type='gpt2', n_gpu=1, no_cuda=False, num_return_sequences=1, p=0.9, padding_text='', prompt='', repetition_penalty=1.0, seed=42, stop_token=None, temperature=1.0, xlm_language='')\n",
            "Model prompt >>> yes\n",
            "=== GENERATED SEQUENCE 1 ===\n",
            "yes when everything started going well for you at only one point and you were there two weeks and all that\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Sl9JaKa9g9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}